import random

#1 - compact dictionary into a dict or dict 414-436 homer simpson
#2 - Change csv simulated dataset to return for the function, change w_est, inside simulation_bnlearn, simulation_notears return instead of np.savetxt
#3 - Compact a single execution of a pipeline into a class 445-764
import importlib

from pgmpy.estimators import BayesianEstimator, PC, HillClimbSearch, BicScore, TreeSearch, MmhcEstimator
from pgmpy.models import BayesianModel
from pomegranate import BayesianNetwork
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB, CategoricalNB, BernoulliNB, MultinomialNB, ComplementNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from statistics import mean
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import csv
import simulation_notears
import simulation_bnlearn
import simulation_dagsim
import simulation_models
from sklearn import metrics
from sklearn import svm
from notears import utils
from notears.linear import notears_linear

#Save linear, nonlinear, sparse, dimensional training set of the real-world for reproducablity
global pipeline_type
global linear_training
global nonlinear_training
global sparse_training
global dimensional_training

TRAIN_SIZE = 1000
TEST_SIZE = 1000


def slice_data(pipeline_type, train_data, test_data):
    if(pipeline_type==4):
        x_train = train_data.iloc[:, 0:10].to_numpy().reshape([-1, 10])  # num predictors
        y_train = train_data.iloc[:, 10].to_numpy().reshape([-1]).ravel()  # outcome
        x_test = test_data.iloc[:, 0:10].to_numpy().reshape([-1, 10])  # num predictors
        y_test = test_data.iloc[:, 10].to_numpy().reshape([-1]).ravel()  # outcome
    else:
        x_train = train_data.iloc[:, 0:4].to_numpy().reshape([-1, 4])  # num predictors
        y_train = train_data.iloc[:, 4].to_numpy().reshape([-1]).ravel()  # outcome
        x_test = test_data.iloc[:, 0:4].to_numpy().reshape([-1, 10])  # num predictors
        y_test = test_data.iloc[:, 4].to_numpy().reshape([-1]).ravel()  # outcome
    return x_train, y_train, x_test, y_test


def get_data_from_real_world(pipeline_type, num_train, num_test, world=None, data=None):
    '''
    Simulate data using dagsim based on the pipeline type
    :param pipeline_type: (str)
    :return: sliced train and test data
    '''
    # if data is None:
    #     if world!="real":
    #             raise("For learning a world, data are needed")
    #     else:
    #         train, test = simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
    # else:
    #     if world=="real":
    #         raise("world==real with data are not allowed")
    #     elif world=="notears":
    #         train, test = simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
    #     elif world=="bnlearn":
    #         train, test = simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
    train, test = simulation_dagsim.setup_realworld(pipeline_type, num_train, num_test)
    x_train, y_train, x_test, y_test = slice_data(pipeline_type, train, test)
    return x_train, y_train, x_test, y_test


def get_data_from_learned_world(pkg_name, config, real_data, num_train, num_test, pipeline_type):
    # module = importlib.import_module(pkg_name)
    # function = getattr(module, f'{pkg_name}_setup_{config}')
    learned_data_train = None
    learned_data_test = None
    if pkg_name=="notears":
        model = notears_linear(real_data[0:100], lambda1=0.01, loss_type=config)
        learned_data_train = utils.simulate_linear_sem(model, num_train, 'logistic')
        learned_data_test = utils.simulate_linear_sem(model, num_test, 'logistic')
    elif pkg_name=="pgmpy":
        model_learn = None
        model = None
        if config=="pc":
            model_learn = PC(real_data[0:100])
            model = model_learn.estimate()
        elif config=="hc":
            model_learn = HillClimbSearch(real_data[0:100])
            model = model_learn.estimate(scoring_method=BicScore(real_data[0:100]))
        elif config=="tree":
            model_learn = TreeSearch(real_data[0:100])
            model = model_learn.estimate(estimator_type='chow-liu')
        elif config=="mmhc":
            model_learn = MmhcEstimator(real_data[0:100])
            model = model_learn.estimate()
        construct = BayesianModel(model)
        estimator = BayesianEstimator(construct, real_data[0:100])
        cpds = estimator.get_parameters(prior_type='BDeu', equivalent_sample_size=1000)
        for cpd in cpds:
            construct.add_cpds(cpd)
        construct.check_model()
        learned_data_train = construct.simulate(n_samples=int(1000))
        learned_data_test = construct.simulate(n_samples=int(1000))
    elif pkg_name=="pomegranate":
        model = BayesianNetwork.from_samples(real_data[0:100], state_names=real_data[0:100].columns.values, algorithm=config)
        learned_data_train = model.sample(1000)
        learned_data_test = model.sample(1000)
    x_train, y_train, x_test, y_test = slice_data(pipeline_type, learned_data_train, learned_data_test)
    return x_train, y_train, x_test, y_test


# Evaluate function for all ML algorithms
def world_evaluate(world, pipeline_type, x_train, y_train, x_test, y_test):
    scores = {}
    pipeline_name = ["linear", "non-linear", "sparse", "dimension"][pipeline_type-1]
    MLModels = {"DTCgini": DecisionTreeClassifier(criterion='gini'),
                "DTCent": DecisionTreeClassifier(criterion='entropy'),
                "RFCgini": RandomForestClassifier(criterion='gini'),
                "RFCent": RandomForestClassifier(criterion='entropy'),
                "LRnone": LogisticRegression(penalty='none'),
                "LRl1": LogisticRegression(penalty='l1', solver='liblinear', l1_ratio=1),
                "LRl2": LogisticRegression(penalty='l2'),
                "LRmix": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),
                "BNB": BernoulliNB(), "GNB": GaussianNB(), "MnNB": MultinomialNB(), "CNB": ComplementNB(),
                "SVMsig": svm.SVC(kernel="sigmoid"), "SVMpoly": svm.SVC(kernel="poly"), "SVMrbf": svm.SVC(kernel="rbf"),
                "KNCunif": KNeighborsClassifier(weights='uniform'), "KNCdist": KNeighborsClassifier(weights='distance')}
    x_train_rdy = x_train
    x_test_rdy = x_test
    for key in MLModels.keys():
        if key in ["MnNB", "CNB"]:
            min_max_scaler = MinMaxScaler()
            x_train_rdy = min_max_scaler.fit_transform(x_train)
            x_test_rdy = min_max_scaler.transform(x_test)
        clf = MLModels[key]
        clf = clf.fit(x_train_rdy, y_train)
        y_pred = clf.predict(x_test)
        scores[f'{world}_{pipeline_name}_{key}'] = metrics.accuracy_score(y_test,y_pred)
    return scores

def evaluate_real(num_train, num_test):
    results = {}
    for pipeline_type in range(1,5):
        x_train, y_train, x_test, y_test = get_data_from_real_world(pipeline_type, num_train, num_test)
        scores = world_evaluate("real", pipeline_type, x_train, y_train, x_test, y_test)
        results.update(scores)
    return results

def evaluate_on_learned_world(x_train, y_train, x_test, y_test):
    learners = ["notears","pgmpy","pomegranate"]
    notears_loss = ["logistic", "l2", "poisson"]
    pgmpy_algorithms = ["hc", "pc", "tree", "mmhc"]
    pomegranate_algorithms = ["exact", "greedy"]
    pipelines = list(range(1, 5))
    results = {}
    for pipeline in pipelines:
        for loss in notears_loss:
            x_train_lr, y_train_lr, _, _ = get_data_from_learned_world("notears", loss, np.concatenate([x_train[:100], y_train[:100]], axis=1), TRAIN_SIZE, TEST_SIZE, pipeline_type)
            scores = world_evaluate("notears", pipeline, x_train_lr, y_train_lr, x_test, y_test)
            results.update(scores)
        for algorithm in pgmpy_algorithms:
            x_train_lr, y_train_lr, _, _ = get_data_from_learned_world("pgmpy", algorithm, np.concatenate([x_train[:100], y_train[:100]], axis=1), TRAIN_SIZE, TEST_SIZE, pipeline_type)
            scores = world_evaluate("pgmpy", pipeline, x_train_lr, y_train_lr, x_test, y_test)
            results.update(scores)
        for algorithm in pomegranate_algorithms:
            x_train_lr, y_train_lr, _, _ = get_data_from_learned_world("pomegranate", algorithm, np.concatenate([x_train[:100], y_train[:100]], axis=1), TRAIN_SIZE, TEST_SIZE, pipeline_type)
            scores = world_evaluate("pomegranate", pipeline, x_train_lr, y_train_lr, x_test, y_test)
            results.update(scores)
    return results

#PIPELINE_TYPE_LINEAR_CONSTANT = 1
#PIPELINE_TYPE_NONLINEAR_CONSTANT = 2
#PIPELINE_TYPE_SPARSE_CONSTANT = 3
#PIPELINE_TYPE_DIMENSION_CONSTANT = 4

#data_real_linear = get_data_from_real_world(PIPELINE_TYPE_LINEAR_CONSTANT, 1000, 1000)
#data_real_nonlinear = get_data_from_real_world(PIPELINE_TYPE_NONLINEAR_CONSTANT, 1000, 1000)
#data_real_sparse = get_data_from_real_world(PIPELINE_TYPE_SPARSE_CONSTANT, 1000, 1000)
#data_real_dimension = get_data_from_real_world(PIPELINE_TYPE_DIMENSION_CONSTANT, 1000, 1000)



def run_all():
    results_real = {}
    results_learned = {}
    for pipeline_type in range(1,5):
        x_train, y_train, x_test, y_test = get_data_from_real_world(pipeline_type, TRAIN_SIZE, TEST_SIZE)
        scores_real = world_evaluate("real", pipeline_type, x_train, y_train, x_test, y_test)
        results_real.update(scores_real)
        scores_learned = evaluate_on_learned_world(x_train, y_train, x_test, y_test)
        results_learned.update(scores_learned)

scores_real = evaluate_real(1000, 1000)
print(scores_real)

# scores_learned = world_evaluate("real", PIPELINE_TYPE_LINEAR_CONSTANT, *data_real_linear)

#get_data_from_learned_world("notears", "logistic", real_data, num_train, num_test, pipeline_type):

#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_hc(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (HC)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_hc(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (HC)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_hc(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (HC)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_hc(train_data[0:100], pipeline_type)
#import_simulated_csv()
#bnlearn_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:10], bn_learn_sample_train.iloc[:,10], pipeline_type, "BN LEARN (HC)")

#Run hyperparameter of bnlearn - tabu
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_tabu(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_tabu_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (TABU)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_tabu(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_tabu_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (TABU)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_tabu(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_tabu_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (TABU)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_tabu(train_data[0:100], pipeline_type)
#import_simulated_csv()
#bnlearn_tabu_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:10], bn_learn_sample_train.iloc[:,10], pipeline_type, "BN LEARN (TABU)")
#end of tabu workflows

#Run hyperparameter of bnlearn - pc
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_pc(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_pc_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (PC)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_pc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed.
#import_simulated_csv()

#bnlearn_pc_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (PC)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_pc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed.
#import_simulated_csv()

#bnlearn_pc_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (PC)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_pc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()
#bnlearn_pc_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:2], bn_learn_sample_train.iloc[:,2], pipeline_type, "BN LEARN (PC)")
#end of pc workflows

#Run hyperparameter of bnlearn - gs
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_gs(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_gs_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (GS)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_gs(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_gs_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (GS)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_gs(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_gs_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (GS)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_gs(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()
#bnlearn_gs_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:2], bn_learn_sample_train.iloc[:,2], pipeline_type, "BN LEARN (GS)")
#end of gs workflows

#Run hyperparameter of bnlearn - iamb
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_iamb(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed.
#import_simulated_csv()

#bnlearn_iamb_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (IAMB)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_iamb(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_iamb_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (IAMB)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_iamb(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_iamb_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (IAMB)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 10000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_iamb(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()
#bnlearn_iamb_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:2], bn_learn_sample_train.iloc[:,2], pipeline_type, "BN LEARN (IAMB)")
#end of pc workflows

#Run hyperparameter of bnlearn - mmhc
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_mmhc(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_mmhc_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (MMHC)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_mmhc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_mmhc_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (MMHC)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_mmhc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_mmhc_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (MMHC)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_mmhc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()
#bnlearn_mmhc_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:10], bn_learn_sample_train.iloc[:,10], pipeline_type, "BN LEARN (MMHC)")
#end of mmhc workflows

#Run hyperparameter of bnlearn - rsmax2
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_rsmax2(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_rsmax2_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (RSMAX2)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_rsmax2(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_rsmax2_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (RSMAX2)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_rsmax2(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_rsmax2_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (RSMAX2)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_rsmax2(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()
#bnlearn_rsmax2_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:10], bn_learn_sample_train.iloc[:,10], pipeline_type, "BN LEARN (RSMAX2)")
#end of rsmax2 workflows

#Run hyperparameter of bnlearn - h2pc
#pipeline_type = 1
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_h2pc(train_data[0:100], pipeline_type)
#import_simulated_csv()

#bnlearn_h2pc_linear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (H2PC)")
#pipeline_type = 2
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_h2pc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_h2pc_nonlinear_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (H2PC)")
#pipeline_type = 3
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_h2pc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()

#bnlearn_h2pc_sparse_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:4], bn_learn_sample_train.iloc[:,4], pipeline_type, "BN LEARN (H2PC)")
#pipeline_type = 4
#simulation_dagsim.setup_realworld(pipeline_type, 1000, 5000)
#import_real_world_csv(pipeline_type)
#simulation_bnlearn.bnlearn_setup_h2pc(train_data[0:100], pipeline_type) #rpy2.rinterface_lib.embedded.RRuntimeError: Error in bn.fit(my_bn, databn) : the graph is only partially directed
#import_simulated_csv()
#bnlearn_h2pc_dimension_dict_scores = run_learned_workflows(bn_learn_sample_train.iloc[:,0:10], bn_learn_sample_train.iloc[:,10], pipeline_type, "BN LEARN (H2PC)")
#end of h2pc workflows

def write_learned_to_csv():
    #for loop with drawn parameters from dict - list comprehension
    #for algorithm in ...
    #for model in ..
    experiments = ['Algorithm', 'Model', 'Linear', 'Non-linear', 'Sparsity', 'Dimensionality']
    with open('simulation_experiments_summary.csv', 'w', newline='') as csvfile:
        fieldnames = ['Algorithm', 'Model', 'Linear', 'Non-linear', 'Sparsity', 'Dimensionality']
        thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)
        thewriter.writeheader()

        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(notears_l2_linear_dict_scores["dt"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["dt"])) + "," + str(max(notears_l2_linear_dict_scores["dt"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["dt"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["dt"])) + "," + str(max(notears_l2_nonlinear_dict_scores["dt"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["dt"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["dt"])) + "," + str(max(notears_l2_sparse_dict_scores["dt"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["dt"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["dt"])) + "," + str(max(notears_l2_dimension_dict_scores["dt"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(notears_l2_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["dt_e"])) + "," + str(max(notears_l2_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["dt_e"])) + "," + str(max(notears_l2_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["dt_e"])) + "," + str(max(notears_l2_sparse_dict_scores["dt_e"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["dt_e"])) + "," + str(max(notears_l2_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(notears_l2_linear_dict_scores["rf"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["rf"])) + "," + str(max(notears_l2_linear_dict_scores["rf"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["rf"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["rf"])) + "," + str(max(notears_l2_nonlinear_dict_scores["rf"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["rf"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["rf"])) + "," + str(max(notears_l2_sparse_dict_scores["rf"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["rf"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["rf"])) + "," + str(max(notears_l2_dimension_dict_scores["rf"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(notears_l2_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["rf_e"])) + "," + str(max(notears_l2_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["rf_e"])) + "," + str(max(notears_l2_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["rf_e"])) + "," + str(max(notears_l2_sparse_dict_scores["rf_e"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["rf_e"])) + "," + str(max(notears_l2_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(notears_l2_linear_dict_scores["lr"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["lr"])) + "," + str(max(notears_l2_linear_dict_scores["lr"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["lr"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["lr"])) + "," + str(max(notears_l2_nonlinear_dict_scores["lr"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["lr"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["lr"])) + "," + str(max(notears_l2_sparse_dict_scores["lr"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["lr"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["lr"])) + "," + str(max(notears_l2_dimension_dict_scores["lr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(notears_l2_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["lr_l1"])) + "," + str(max(notears_l2_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["lr_l1"])) + "," + str(max(notears_l2_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["lr_l1"])) + "," + str(max(notears_l2_sparse_dict_scores["lr_l1"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["lr_l1"])) + "," + str(max(notears_l2_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(notears_l2_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["lr_l2"])) + "," + str(max(notears_l2_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["lr_l2"])) + "," + str(max(notears_l2_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["lr_l2"])) + "," + str(max(notears_l2_sparse_dict_scores["lr_l2"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["lr_l2"])) + "," + str(max(notears_l2_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(notears_l2_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["lr_e"])) + "," + str(max(notears_l2_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["lr_e"])) + "," + str(max(notears_l2_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["lr_e"])) + "," + str(max(notears_l2_sparse_dict_scores["lr_e"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["lr_e"])) + "," + str(max(notears_l2_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(notears_l2_linear_dict_scores["nb"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["nb"])) + "," + str(max(notears_l2_linear_dict_scores["nb"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["nb"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["nb"])) + "," + str(max(notears_l2_nonlinear_dict_scores["nb"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["nb"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["nb"])) + "," + str(max(notears_l2_sparse_dict_scores["nb"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["nb"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["nb"])) + "," + str(max(notears_l2_dimension_dict_scores["nb"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(notears_l2_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["nb_m"])) + "," + str(max(notears_l2_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["nb_m"])) + "," + str(max(notears_l2_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["nb_m"])) + "," + str(max(notears_l2_sparse_dict_scores["nb_m"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["nb_m"])) + "," + str(max(notears_l2_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(notears_l2_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["nb_g"])) + "," + str(max(notears_l2_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["nb_g"])) + "," + str(max(notears_l2_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["nb_g"])) + "," + str(max(notears_l2_sparse_dict_scores["nb_g"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["nb_g"])) + "," + str(max(notears_l2_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(notears_l2_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["nb_c"])) + "," + str(max(notears_l2_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["nb_c"])) + "," + str(max(notears_l2_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["nb_c"])) + "," + str(max(notears_l2_sparse_dict_scores["nb_c"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["nb_c"])) + "," + str(max(notears_l2_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(notears_l2_linear_dict_scores["svm"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["svm"])) + "," + str(max(notears_l2_linear_dict_scores["svm"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["svm"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["svm"])) + "," + str(max(notears_l2_nonlinear_dict_scores["svm"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["svm"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["svm"])) + "," + str(max(notears_l2_sparse_dict_scores["svm"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["svm"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["svm"])) + "," + str(max(notears_l2_dimension_dict_scores["svm"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(notears_l2_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["svm_l"])) + "," + str(max(notears_l2_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["svm_l"])) + "," + str(max(notears_l2_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["svm_l"])) + "," + str(max(notears_l2_sparse_dict_scores["svm_l"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["svm_l"])) + "," + str(max(notears_l2_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(notears_l2_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["svm_po"])) + "," + str(max(notears_l2_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["svm_po"])) + "," + str(max(notears_l2_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["svm_po"])) + "," + str(max(notears_l2_sparse_dict_scores["svm_po"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["svm_po"])) + "," + str(max(notears_l2_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(notears_l2_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["svm_r"])) + "," + str(max(notears_l2_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["svm_r"])) + "," + str(max(notears_l2_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["svm_r"])) + "," + str(max(notears_l2_sparse_dict_scores["svm_r"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["svm_r"])) + "," + str(max(notears_l2_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(notears_l2_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["svm_pr"])) + "," + str(max(notears_l2_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["svm_pr"])) + "," + str(max(notears_l2_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["svm_pr"])) + "," + str(max(notears_l2_sparse_dict_scores["svm_pr"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["svm_pr"])) + "," + str(max(notears_l2_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(notears_l2_linear_dict_scores["knn"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["knn"])) + "," + str(max(notears_l2_linear_dict_scores["knn"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["knn"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["knn"])) + "," + str(max(notears_l2_nonlinear_dict_scores["knn"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["knn"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["knn"])) + "," + str(max(notears_l2_sparse_dict_scores["knn"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["knn"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["knn"])) + "," + str(max(notears_l2_dimension_dict_scores["knn"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-L2)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(notears_l2_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_l2_linear_dict_scores["knn_d"])) + "," + str(max(notears_l2_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(notears_l2_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_l2_nonlinear_dict_scores["knn_d"])) + "," + str(max(notears_l2_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(notears_l2_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_l2_sparse_dict_scores["knn_d"])) + "," + str(max(notears_l2_sparse_dict_scores["knn_d"])) + "}",'Dimensionality': str(round(mean(notears_l2_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_l2_dimension_dict_scores["knn_d"])) + "," + str(max(notears_l2_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(notears_linear_dict_scores["dt"]),2))+" {"+str(min(notears_linear_dict_scores["dt"]))+","+str(max(notears_linear_dict_scores["dt"]))+"}", 'Non-linear': str(round(mean(notears_nonlinear_dict_scores["dt"]),2))+" {"+str(min(notears_nonlinear_dict_scores["dt"]))+","+str(max(notears_nonlinear_dict_scores["dt"]))+"}", 'Sparsity': str(round(mean(notears_sparse_dict_scores["dt"]),2))+" {"+str(min(notears_sparse_dict_scores["dt"]))+","+str(max(notears_sparse_dict_scores["dt"]))+"}", 'Dimensionality': str(round(mean(notears_dimension_dict_scores["dt"]),2))+" {"+str(min(notears_dimension_dict_scores["dt"]))+","+str(max(notears_dimension_dict_scores["dt"]))+"}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(notears_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_linear_dict_scores["dt_e"])) + "," + str(max(notears_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["dt_e"])) + "," + str(max(notears_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_sparse_dict_scores["dt_e"])) + "," + str(max(notears_sparse_dict_scores["dt_e"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_dimension_dict_scores["dt_e"])) + "," + str(max(notears_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Random Forest (gini)', 'Linear': str(round(mean(notears_linear_dict_scores["rf"]),2))+" {"+str(min(notears_linear_dict_scores["rf"]))+","+str(max(notears_linear_dict_scores["rf"]))+"}", 'Non-linear': str(round(mean(notears_nonlinear_dict_scores["rf"]),2))+" {"+str(min(notears_nonlinear_dict_scores["rf"]))+","+str(max(notears_nonlinear_dict_scores["rf"]))+"}", 'Sparsity': str(round(mean(notears_sparse_dict_scores["rf"]),2))+" {"+str(min(notears_sparse_dict_scores["rf"]))+","+str(max(notears_sparse_dict_scores["rf"]))+"}", 'Dimensionality': str(round(mean(notears_dimension_dict_scores["rf"]),2))+" {"+str(min(notears_dimension_dict_scores["rf"]))+","+str(max(notears_dimension_dict_scores["rf"]))+"}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(notears_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_linear_dict_scores["rf_e"])) + "," + str(max(notears_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["rf_e"])) + "," + str(max(notears_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_sparse_dict_scores["rf_e"])) + "," + str(max(notears_sparse_dict_scores["rf_e"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_dimension_dict_scores["rf_e"])) + "," + str(max(notears_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Logistic Regression (penalty-none)', 'Linear': str(round(mean(notears_linear_dict_scores["lr"]),2))+" {"+str(min(notears_linear_dict_scores["lr"]))+","+str(max(notears_linear_dict_scores["lr"]))+"}", 'Non-linear': str(round(mean(notears_nonlinear_dict_scores["lr"]),2))+" {"+str(min(notears_nonlinear_dict_scores["lr"]))+","+str(max(notears_nonlinear_dict_scores["lr"]))+"}", 'Sparsity': str(round(mean(notears_sparse_dict_scores["lr"]),2))+" {"+str(min(notears_sparse_dict_scores["lr"]))+","+str(max(notears_sparse_dict_scores["lr"]))+"}", 'Dimensionality': str(round(mean(notears_dimension_dict_scores["lr"]),2))+" {"+str(min(notears_dimension_dict_scores["lr"]))+","+str(max(notears_dimension_dict_scores["lr"]))+"}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(notears_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_linear_dict_scores["lr_l1"])) + "," + str(max(notears_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["lr_l1"])) + "," + str(max(notears_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_sparse_dict_scores["lr_l1"])) + "," + str(max(notears_sparse_dict_scores["lr_l1"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_dimension_dict_scores["lr_l1"])) + "," + str(max(notears_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(notears_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_linear_dict_scores["lr_l2"])) + "," + str(max(notears_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["lr_l2"])) + "," + str(max(notears_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_sparse_dict_scores["lr_l2"])) + "," + str(max(notears_sparse_dict_scores["lr_l2"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_dimension_dict_scores["lr_l2"])) + "," + str(max(notears_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(notears_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_linear_dict_scores["lr_e"])) + "," + str(max(notears_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["lr_e"])) + "," + str(max(notears_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_sparse_dict_scores["lr_e"])) + "," + str(max(notears_sparse_dict_scores["lr_e"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_dimension_dict_scores["lr_e"])) + "," + str(max(notears_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Naive Bayes (Bernoulli)', 'Linear': str(round(mean(notears_linear_dict_scores["nb"]),2))+" {"+str(min(notears_linear_dict_scores["nb"]))+","+str(max(notears_linear_dict_scores["nb"]))+"}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["nb"]),2))+" {"+str(min(notears_nonlinear_dict_scores["nb"]))+","+str(max(notears_nonlinear_dict_scores["nb"]))+"}", 'Sparsity': str(round(mean(notears_sparse_dict_scores["nb"]),2))+" {"+str(min(notears_sparse_dict_scores["nb"]))+","+str(max(notears_sparse_dict_scores["nb"]))+"}", 'Dimensionality': str(round(mean(notears_dimension_dict_scores["nb"]),2))+" {"+str(min(notears_dimension_dict_scores["nb"]))+","+str(max(notears_dimension_dict_scores["nb"]))+"}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(notears_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_linear_dict_scores["nb_m"])) + "," + str(max(notears_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["nb_m"])) + "," + str(max(notears_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_sparse_dict_scores["nb_m"])) + "," + str(max(notears_sparse_dict_scores["nb_m"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_dimension_dict_scores["nb_m"])) + "," + str(max(notears_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(notears_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_linear_dict_scores["nb_g"])) + "," + str(max(notears_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["nb_g"])) + "," + str(max(notears_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_sparse_dict_scores["nb_g"])) + "," + str(max(notears_sparse_dict_scores["nb_g"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_dimension_dict_scores["nb_g"])) + "," + str(max(notears_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(notears_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_linear_dict_scores["nb_c"])) + "," + str(max(notears_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["nb_c"])) + "," + str(max(notears_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_sparse_dict_scores["nb_c"])) + "," + str(max(notears_sparse_dict_scores["nb_c"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_dimension_dict_scores["nb_c"])) + "," + str(max(notears_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Support Vector Machines (sigmoid)', 'Linear': str(round(mean(notears_linear_dict_scores["svm"]),2))+" {"+str(min(notears_linear_dict_scores["svm"]))+","+str(max(notears_linear_dict_scores["svm"]))+"}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["svm"]),2))+" {"+str(min(notears_nonlinear_dict_scores["svm"]))+","+str(max(notears_nonlinear_dict_scores["svm"]))+"}", 'Sparsity': str(round(mean(notears_sparse_dict_scores["svm"]),2))+" {"+str(min(notears_sparse_dict_scores["svm"]))+","+str(max(notears_sparse_dict_scores["svm"]))+"}", 'Dimensionality': str(round(mean(notears_dimension_dict_scores["svm"]),2))+" {"+str(min(notears_dimension_dict_scores["svm"]))+","+str(max(notears_dimension_dict_scores["svm"]))+"}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(notears_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_linear_dict_scores["svm_l"])) + "," + str(max(notears_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["svm_l"])) + "," + str(max(notears_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_sparse_dict_scores["svm_l"])) + "," + str(max(notears_sparse_dict_scores["svm_l"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_dimension_dict_scores["svm_l"])) + "," + str(max(notears_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(notears_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_linear_dict_scores["svm_po"])) + "," + str(max(notears_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["svm_po"])) + "," + str(max(notears_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_sparse_dict_scores["svm_po"])) + "," + str(max(notears_sparse_dict_scores["svm_po"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_dimension_dict_scores["svm_po"])) + "," + str(max(notears_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(notears_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_linear_dict_scores["svm_r"])) + "," + str(max(notears_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["svm_r"])) + "," + str(max(notears_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_sparse_dict_scores["svm_r"])) + "," + str(max(notears_sparse_dict_scores["svm_r"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_dimension_dict_scores["svm_r"])) + "," + str(max(notears_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(notears_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_linear_dict_scores["svm_pr"])) + "," + str(max(notears_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["svm_pr"])) + "," + str(max(notears_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_sparse_dict_scores["svm_pr"])) + "," + str(max(notears_sparse_dict_scores["svm_pr"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_dimension_dict_scores["svm_pr"])) + "," + str(max(notears_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'K Nearest Neighbor (uniform)', 'Linear': str(round(mean(notears_linear_dict_scores["knn"]),2))+" {"+str(min(notears_linear_dict_scores["knn"]))+","+str(max(notears_linear_dict_scores["knn"]))+"}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["knn"]),2))+" {"+str(min(notears_nonlinear_dict_scores["knn"]))+","+str(max(notears_nonlinear_dict_scores["knn"]))+"}", 'Sparsity': str(round(mean(notears_sparse_dict_scores["knn"]),2))+" {"+str(min(notears_sparse_dict_scores["knn"]))+","+str(max(notears_sparse_dict_scores["knn"]))+"}", 'Dimensionality': str(round(mean(notears_dimension_dict_scores["knn"]),2))+" {"+str(min(notears_dimension_dict_scores["knn"]))+","+str(max(notears_dimension_dict_scores["knn"]))+"}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Logistic)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(notears_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_linear_dict_scores["knn_d"])) + "," + str(max(notears_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(notears_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_nonlinear_dict_scores["knn_d"])) + "," + str(max(notears_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(notears_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_sparse_dict_scores["knn_d"])) + "," + str(max(notears_sparse_dict_scores["knn_d"])) + "}",'Dimensionality': str(round(mean(notears_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_dimension_dict_scores["knn_d"])) + "," + str(max(notears_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(notears_poisson_linear_dict_scores["dt"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["dt"])) + "," + str(max(notears_poisson_linear_dict_scores["dt"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["dt"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["dt"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["dt"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["dt"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["dt"])) + "," + str(max(notears_poisson_sparse_dict_scores["dt"])) + "}",'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["dt"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["dt"])) + "," + str(max(notears_poisson_dimension_dict_scores["dt"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(notears_poisson_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["dt_e"])) + "," + str(max(notears_poisson_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["dt_e"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["dt_e"])) + "," + str(max(notears_poisson_sparse_dict_scores["dt_e"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["dt_e"])) + "," + str(max(notears_poisson_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(notears_poisson_linear_dict_scores["rf"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["rf"])) + "," + str(max(notears_poisson_linear_dict_scores["rf"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["rf"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["rf"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["rf"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["rf"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["rf"])) + "," + str(max(notears_poisson_sparse_dict_scores["rf"])) + "}",'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["rf"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["rf"])) + "," + str(max(notears_poisson_dimension_dict_scores["rf"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(notears_poisson_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["rf_e"])) + "," + str(max(notears_poisson_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["rf_e"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["rf_e"])) + "," + str(max(notears_poisson_sparse_dict_scores["rf_e"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["rf_e"])) + "," + str(max(notears_poisson_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(notears_poisson_linear_dict_scores["lr"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["lr"])) + "," + str(max(notears_poisson_linear_dict_scores["lr"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["lr"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["lr"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["lr"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["lr"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["lr"])) + "," + str(max(notears_poisson_sparse_dict_scores["lr"])) + "}",'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["lr"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["lr"])) + "," + str(max(notears_poisson_dimension_dict_scores["lr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(notears_poisson_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["lr_l1"])) + "," + str(max(notears_poisson_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["lr_l1"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["lr_l1"])) + "," + str(max(notears_poisson_sparse_dict_scores["lr_l1"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["lr_l1"])) + "," + str(max(notears_poisson_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(notears_poisson_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["lr_l2"])) + "," + str(max(notears_poisson_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["lr_l2"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["lr_l2"])) + "," + str(max(notears_poisson_sparse_dict_scores["lr_l2"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["lr_l2"])) + "," + str(max(notears_poisson_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(notears_poisson_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["lr_e"])) + "," + str(max(notears_poisson_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["lr_e"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["lr_e"])) + "," + str(max(notears_poisson_sparse_dict_scores["lr_e"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["lr_e"])) + "," + str(max(notears_poisson_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(notears_poisson_linear_dict_scores["nb"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["nb"])) + "," + str(max(notears_poisson_linear_dict_scores["nb"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["nb"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["nb"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["nb"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["nb"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["nb"])) + "," + str(max(notears_poisson_sparse_dict_scores["nb"])) + "}",'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["nb"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["nb"])) + "," + str(max(notears_poisson_dimension_dict_scores["nb"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(notears_poisson_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["nb_m"])) + "," + str(max(notears_poisson_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["nb_m"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["nb_m"])) + "," + str(max(notears_poisson_sparse_dict_scores["nb_m"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["nb_m"])) + "," + str(max(notears_poisson_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(notears_poisson_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["nb_g"])) + "," + str(max(notears_poisson_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["nb_g"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["nb_g"])) + "," + str(max(notears_poisson_sparse_dict_scores["nb_g"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["nb_g"])) + "," + str(max(notears_poisson_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(notears_poisson_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["nb_c"])) + "," + str(max(notears_poisson_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["nb_c"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["nb_c"])) + "," + str(max(notears_poisson_sparse_dict_scores["nb_c"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["nb_c"])) + "," + str(max(notears_poisson_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(notears_poisson_linear_dict_scores["svm"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["svm"])) + "," + str(max(notears_poisson_linear_dict_scores["svm"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["svm"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["svm"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["svm"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["svm"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["svm"])) + "," + str(max(notears_poisson_sparse_dict_scores["svm"])) + "}",'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["svm"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["svm"])) + "," + str(max(notears_poisson_dimension_dict_scores["svm"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(notears_poisson_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["svm_l"])) + "," + str(max(notears_poisson_linear_dict_scores["svm_l"])) + "}", 'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["svm_l"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["svm_l"])) + "," + str(max(notears_poisson_sparse_dict_scores["svm_l"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["svm_l"])) + "," + str(max(notears_poisson_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(notears_poisson_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["svm_po"])) + "," + str(max(notears_poisson_linear_dict_scores["svm_po"])) + "}", 'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["svm_po"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["svm_po"])) + "," + str(max(notears_poisson_sparse_dict_scores["svm_po"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["svm_po"])) + "," + str(max(notears_poisson_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(notears_poisson_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["svm_r"])) + "," + str(max(notears_poisson_linear_dict_scores["svm_r"])) + "}", 'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["svm_r"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["svm_r"])) + "," + str(max(notears_poisson_sparse_dict_scores["svm_r"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["svm_r"])) + "," + str(max(notears_poisson_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(notears_poisson_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["svm_pr"])) + "," + str(max(notears_poisson_linear_dict_scores["svm_pr"])) + "}", 'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["svm_pr"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["svm_pr"])) + "," + str(max(notears_poisson_sparse_dict_scores["svm_pr"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["svm_pr"])) + "," + str(max(notears_poisson_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(notears_poisson_linear_dict_scores["knn"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["knn"])) + "," + str(max(notears_poisson_linear_dict_scores["knn"])) + "}",'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["knn"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["knn"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["knn"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["knn"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["knn"])) + "," + str(max(notears_poisson_sparse_dict_scores["knn"])) + "}",'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["knn"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["knn"])) + "," + str(max(notears_poisson_dimension_dict_scores["knn"])) + "}"})
        #thewriter.writerow({'Algorithm': 'NO TEARS (Loss-Poisson)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(notears_poisson_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_poisson_linear_dict_scores["knn_d"])) + "," + str(max(notears_poisson_linear_dict_scores["knn_d"])) + "}", 'Non-linear': str(round(mean(notears_poisson_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_poisson_nonlinear_dict_scores["knn_d"])) + "," + str(max(notears_poisson_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(notears_poisson_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_poisson_sparse_dict_scores["knn_d"])) + "," + str(max(notears_poisson_sparse_dict_scores["knn_d"])) + "}", 'Dimensionality': str(round(mean(notears_poisson_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(notears_poisson_dimension_dict_scores["knn_d"])) + "," + str(max(notears_poisson_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Decision Tree (gini)', 'Linear': str(round(mean(bnlearn_linear_dict_scores["dt"]),2))+" {"+str(min(bnlearn_linear_dict_scores["dt"]))+","+str(max(bnlearn_linear_dict_scores["dt"]))+"}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["dt"]),2))+" {"+str(min(bnlearn_nonlinear_dict_scores["dt"]))+","+str(max(bnlearn_nonlinear_dict_scores["dt"]))+"}", 'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["dt"]),2))+" {"+str(min(bnlearn_sparse_dict_scores["dt"]))+","+str(max(bnlearn_sparse_dict_scores["dt"]))+"}", 'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["dt"]),2))+" {"+str(min(bnlearn_dimension_dict_scores["dt"]))+","+str(max(bnlearn_dimension_dict_scores["dt"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["dt_e"])) + "," + str(max(bnlearn_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["dt_e"])) + "," + str(max(bnlearn_sparse_dict_scores["dt_e"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["dt_e"])) + "," + str(max(bnlearn_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Random Forest (gini)', 'Linear': str(round(mean(bnlearn_linear_dict_scores["rf"]),2))+" {"+str(min(bnlearn_linear_dict_scores["rf"]))+","+str(max(bnlearn_linear_dict_scores["rf"]))+"}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["rf"]),2))+" {"+str(min(bnlearn_nonlinear_dict_scores["rf"]))+","+str(max(bnlearn_nonlinear_dict_scores["rf"]))+"}", 'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["rf"]),2))+" {"+str(min(bnlearn_sparse_dict_scores["rf"]))+","+str(max(bnlearn_sparse_dict_scores["rf"]))+"}", 'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["rf"]),2))+" {"+str(min(bnlearn_dimension_dict_scores["rf"]))+","+str(max(bnlearn_dimension_dict_scores["rf"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["rf_e"])) + "," + str(max(bnlearn_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["rf_e"])) + "," + str(max(bnlearn_sparse_dict_scores["rf_e"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["rf_e"])) + "," + str(max(bnlearn_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Logistic Regression (penalty-none)', 'Linear': str(round(mean(bnlearn_linear_dict_scores["lr"]),2))+" {"+str(min(bnlearn_linear_dict_scores["lr"]))+","+str(max(bnlearn_linear_dict_scores["lr"]))+"}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["lr"]),2))+" {"+str(min(bnlearn_nonlinear_dict_scores["lr"]))+","+str(max(bnlearn_nonlinear_dict_scores["lr"]))+"}", 'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["lr"]),2))+" {"+str(min(bnlearn_sparse_dict_scores["lr"]))+","+str(max(bnlearn_sparse_dict_scores["lr"]))+"}", 'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["lr"]),2))+" {"+str(min(bnlearn_dimension_dict_scores["lr"]))+","+str(max(bnlearn_dimension_dict_scores["lr"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["lr_l1"])) + "," + str(max(bnlearn_sparse_dict_scores["lr_l1"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["lr_l1"])) + "," + str(max(bnlearn_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["lr_l2"])) + "," + str(max(bnlearn_sparse_dict_scores["lr_l2"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["lr_l2"])) + "," + str(max(bnlearn_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["lr_e"])) + "," + str(max(bnlearn_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["lr_e"])) + "," + str(max(bnlearn_sparse_dict_scores["lr_e"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["lr_e"])) + "," + str(max(bnlearn_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Naive Bayes (Bernoulli)', 'Linear': str(round(mean(bnlearn_linear_dict_scores["nb"]),2))+" {"+str(min(bnlearn_linear_dict_scores["nb"]))+","+str(max(bnlearn_linear_dict_scores["nb"]))+"}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["nb"]),2))+" {"+str(min(bnlearn_nonlinear_dict_scores["nb"]))+","+str(max(bnlearn_nonlinear_dict_scores["nb"]))+"}", 'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["nb"]),2))+" {"+str(min(bnlearn_sparse_dict_scores["nb"]))+","+str(max(bnlearn_sparse_dict_scores["nb"]))+"}", 'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["nb"]),2))+" {"+str(min(bnlearn_dimension_dict_scores["nb"]))+","+str(max(bnlearn_dimension_dict_scores["nb"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["nb_m"])) + "," + str(max(bnlearn_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["nb_m"])) + "," + str(max(bnlearn_sparse_dict_scores["nb_m"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["nb_m"])) + "," + str(max(bnlearn_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["nb_g"])) + "," + str(max(bnlearn_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["nb_g"])) + "," + str(max(bnlearn_sparse_dict_scores["nb_g"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["nb_g"])) + "," + str(max(bnlearn_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["nb_c"])) + "," + str(max(bnlearn_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["nb_c"])) + "," + str(max(bnlearn_sparse_dict_scores["nb_c"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["nb_c"])) + "," + str(max(bnlearn_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Support Vector Machines (sigmoid)', 'Linear': str(round(mean(bnlearn_linear_dict_scores["svm"]),2))+" {"+str(min(bnlearn_linear_dict_scores["svm"]))+","+str(max(bnlearn_linear_dict_scores["svm"]))+"}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["svm"]),2))+" {"+str(min(bnlearn_nonlinear_dict_scores["svm"]))+","+str(max(bnlearn_nonlinear_dict_scores["svm"]))+"}", 'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["svm"]),2))+" {"+str(min(bnlearn_sparse_dict_scores["svm"]))+","+str(max(bnlearn_sparse_dict_scores["svm"]))+"}", 'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["svm"]),2))+" {"+str(min(bnlearn_dimension_dict_scores["svm"]))+","+str(max(bnlearn_dimension_dict_scores["svm"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["svm_l"])) + "," + str(max(bnlearn_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["svm_l"])) + "," + str(max(bnlearn_sparse_dict_scores["svm_l"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["svm_l"])) + "," + str(max(bnlearn_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["svm_po"])) + "," + str(max(bnlearn_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["svm_po"])) + "," + str(max(bnlearn_sparse_dict_scores["svm_po"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["svm_po"])) + "," + str(max(bnlearn_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["svm_r"])) + "," + str(max(bnlearn_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["svm_r"])) + "," + str(max(bnlearn_sparse_dict_scores["svm_r"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["svm_r"])) + "," + str(max(bnlearn_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["svm_pr"])) + "," + str(max(bnlearn_sparse_dict_scores["svm_pr"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["svm_pr"])) + "," + str(max(bnlearn_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'K Nearest Neighbor (uniform)', 'Linear': str(round(mean(bnlearn_linear_dict_scores["knn"]),2))+" {"+str(min(bnlearn_linear_dict_scores["knn"]))+","+str(max(bnlearn_linear_dict_scores["knn"]))+"}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["knn"]),2))+" {"+str(min(bnlearn_nonlinear_dict_scores["knn"]))+","+str(max(bnlearn_nonlinear_dict_scores["knn"]))+"}", 'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["knn"]),2))+" {"+str(min(bnlearn_sparse_dict_scores["knn"]))+","+str(max(bnlearn_sparse_dict_scores["knn"]))+"}", 'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["knn"]),2))+" {"+str(min(bnlearn_dimension_dict_scores["knn"]))+","+str(max(bnlearn_dimension_dict_scores["knn"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (HC)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(bnlearn_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_nonlinear_dict_scores["knn_d"])) + "," + str(max(bnlearn_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(bnlearn_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_sparse_dict_scores["knn_d"])) + "," + str(max(bnlearn_sparse_dict_scores["knn_d"])) + "}",'Dimensionality': str(round(mean(bnlearn_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_dimension_dict_scores["knn_d"])) + "," + str(max(bnlearn_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Decision Tree (gini)', 'Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["dt"]),2))+" {"+str(min(bnlearn_tabu_linear_dict_scores["dt"]))+","+str(max(bnlearn_tabu_linear_dict_scores["dt"]))+"}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["dt"]),2))+" {"+str(min(bnlearn_tabu_nonlinear_dict_scores["dt"]))+","+str(max(bnlearn_tabu_nonlinear_dict_scores["dt"]))+"}", 'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["dt"]),2))+" {"+str(min(bnlearn_tabu_sparse_dict_scores["dt"]))+","+str(max(bnlearn_tabu_sparse_dict_scores["dt"]))+"}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["dt"]),2))+" {"+str(min(bnlearn_tabu_dimension_dict_scores["dt"]))+","+str(max(bnlearn_tabu_dimension_dict_scores["dt"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["dt_e"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["dt_e"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["dt_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["dt_e"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Random Forest (gini)', 'Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["rf"]),2))+" {"+str(min(bnlearn_tabu_linear_dict_scores["rf"]))+","+str(max(bnlearn_tabu_linear_dict_scores["rf"]))+"}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["rf"]),2))+" {"+str(min(bnlearn_tabu_nonlinear_dict_scores["rf"]))+","+str(max(bnlearn_tabu_nonlinear_dict_scores["rf"]))+"}", 'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["rf"]),2))+" {"+str(min(bnlearn_tabu_sparse_dict_scores["rf"]))+","+str(max(bnlearn_tabu_sparse_dict_scores["rf"]))+"}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["rf"]),2))+" {"+str(min(bnlearn_tabu_dimension_dict_scores["rf"]))+","+str(max(bnlearn_tabu_dimension_dict_scores["rf"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["rf_e"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["rf_e"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["rf_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["rf_e"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Logistic Regression (penalty-none)', 'Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["lr"]),2))+" {"+str(min(bnlearn_tabu_linear_dict_scores["lr"]))+","+str(max(bnlearn_tabu_linear_dict_scores["lr"]))+"}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["lr"]),2))+" {"+str(min(bnlearn_tabu_nonlinear_dict_scores["lr"]))+","+str(max(bnlearn_tabu_nonlinear_dict_scores["lr"]))+"}", 'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["lr"]),2))+" {"+str(min(bnlearn_tabu_sparse_dict_scores["lr"]))+","+str(max(bnlearn_tabu_sparse_dict_scores["lr"]))+"}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["lr"]),2))+" {"+str(min(bnlearn_tabu_dimension_dict_scores["lr"]))+","+str(max(bnlearn_tabu_dimension_dict_scores["lr"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["lr_l1"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["lr_l1"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["lr_l1"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["lr_l2"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["lr_l2"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["lr_l2"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["lr_e"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["lr_e"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["lr_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["lr_e"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Naive Bayes (Bernoulli)', 'Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["nb"]),2))+" {"+str(min(bnlearn_tabu_linear_dict_scores["nb"]))+","+str(max(bnlearn_tabu_linear_dict_scores["nb"]))+"}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["nb"]),2))+" {"+str(min(bnlearn_tabu_nonlinear_dict_scores["nb"]))+","+str(max(bnlearn_tabu_nonlinear_dict_scores["nb"]))+"}", 'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["nb"]),2))+" {"+str(min(bnlearn_tabu_sparse_dict_scores["nb"]))+","+str(max(bnlearn_tabu_sparse_dict_scores["nb"]))+"}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["nb"]),2))+" {"+str(min(bnlearn_tabu_dimension_dict_scores["nb"]))+","+str(max(bnlearn_tabu_dimension_dict_scores["nb"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["nb_m"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["nb_m"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["nb_m"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["nb_m"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["nb_g"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["nb_g"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["nb_g"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["nb_g"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["nb_c"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["nb_c"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["nb_c"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["nb_c"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Support Vector Machines (sigmoid)', 'Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["svm"]),2))+" {"+str(min(bnlearn_tabu_linear_dict_scores["svm"]))+","+str(max(bnlearn_tabu_linear_dict_scores["svm"]))+"}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["svm"]),2))+" {"+str(min(bnlearn_tabu_nonlinear_dict_scores["svm"]))+","+str(max(bnlearn_tabu_nonlinear_dict_scores["svm"]))+"}", 'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["svm"]),2))+" {"+str(min(bnlearn_tabu_sparse_dict_scores["svm"]))+","+str(max(bnlearn_tabu_sparse_dict_scores["svm"]))+"}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["svm"]),2))+" {"+str(min(bnlearn_tabu_dimension_dict_scores["svm"]))+","+str(max(bnlearn_tabu_dimension_dict_scores["svm"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["svm_l"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["svm_l"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["svm_l"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["svm_l"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["svm_po"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["svm_po"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["svm_po"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["svm_po"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["svm_r"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["svm_r"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["svm_r"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["svm_r"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["svm_pr"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["svm_pr"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["svm_pr"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'K Nearest Neighbor (uniform)', 'Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["knn"]),2))+" {"+str(min(bnlearn_tabu_linear_dict_scores["knn"]))+","+str(max(bnlearn_tabu_linear_dict_scores["knn"]))+"}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["knn"]),2))+" {"+str(min(bnlearn_tabu_nonlinear_dict_scores["knn"]))+","+str(max(bnlearn_tabu_nonlinear_dict_scores["knn"]))+"}", 'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["knn"]),2))+" {"+str(min(bnlearn_tabu_sparse_dict_scores["knn"]))+","+str(max(bnlearn_tabu_sparse_dict_scores["knn"]))+"}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["knn"]),2))+" {"+str(min(bnlearn_tabu_dimension_dict_scores["knn"]))+","+str(max(bnlearn_tabu_dimension_dict_scores["knn"]))+"}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (TABU)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_tabu_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_tabu_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_tabu_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(bnlearn_tabu_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_tabu_nonlinear_dict_scores["knn_d"])) + "," + str(max(bnlearn_tabu_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(bnlearn_tabu_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_tabu_sparse_dict_scores["knn_d"])) + "," + str(max(bnlearn_tabu_sparse_dict_scores["knn_d"])) + "}", 'Dimensionality': str(round(mean(bnlearn_tabu_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_tabu_dimension_dict_scores["knn_d"])) + "," + str(max(bnlearn_tabu_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["dt"])) + "," + str(max(bnlearn_pc_linear_dict_scores["dt"])) + "}",'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_pc_linear_dict_scores["dt_e"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["rf"])) + "," + str(max(bnlearn_pc_linear_dict_scores["rf"])) + "}",'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_pc_linear_dict_scores["rf_e"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["lr"])) + "," + str(max(bnlearn_pc_linear_dict_scores["lr"])) + "}",'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_pc_linear_dict_scores["lr_l1"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_pc_linear_dict_scores["lr_l2"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_pc_linear_dict_scores["lr_e"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["nb"])) + "," + str(max(bnlearn_pc_linear_dict_scores["nb"])) + "}",'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_pc_linear_dict_scores["nb_m"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_pc_linear_dict_scores["nb_g"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_pc_linear_dict_scores["nb_c"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["svm"])) + "," + str(max(bnlearn_pc_linear_dict_scores["svm"])) + "}",'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_pc_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(bnlearn_pc_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_pc_nonlinear_dict_scores["svm_l"])) + "," + str(max(bnlearn_pc_nonlinear_dict_scores["svm_l"])) + "}", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_pc_linear_dict_scores["svm_po"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_pc_linear_dict_scores["svm_r"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_pc_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(bnlearn_pc_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_pc_nonlinear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_pc_nonlinear_dict_scores["svm_pr"])) + "}", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["knn"])) + "," + str(max(bnlearn_pc_linear_dict_scores["knn"])) + "}",'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (PC)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_pc_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_pc_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_pc_linear_dict_scores["knn_d"])) + "}",'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["dt"])) + "," + str(max(bnlearn_gs_linear_dict_scores["dt"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_gs_linear_dict_scores["dt_e"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["rf"])) + "," + str(max(bnlearn_gs_linear_dict_scores["rf"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_gs_linear_dict_scores["rf_e"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["lr"])) + "," + str(max(bnlearn_gs_linear_dict_scores["lr"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_gs_linear_dict_scores["lr_l1"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_gs_linear_dict_scores["lr_l2"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_gs_linear_dict_scores["lr_e"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["nb"])) + "," + str(max(bnlearn_gs_linear_dict_scores["nb"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_gs_linear_dict_scores["nb_m"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_gs_linear_dict_scores["nb_g"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_gs_linear_dict_scores["nb_c"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["svm"])) + "," + str(max(bnlearn_gs_linear_dict_scores["svm"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_gs_linear_dict_scores["svm_l"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_gs_linear_dict_scores["svm_po"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_gs_linear_dict_scores["svm_r"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_gs_linear_dict_scores["svm_pr"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["knn"])) + "," + str(max(bnlearn_gs_linear_dict_scores["knn"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (GS)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_gs_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_gs_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_gs_linear_dict_scores["knn_d"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["dt"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["dt"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["dt_e"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["rf"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["rf"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["rf_e"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["lr"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["lr"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["lr_l1"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["lr_l2"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["lr_e"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["nb"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["nb"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["nb_m"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["nb_g"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["nb_c"])) + "}", 'Non-linear': "NA", 'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["svm"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["svm"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["svm_l"])) + "}", 'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["svm_po"])) + "}", 'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["svm_r"])) + "}", 'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["svm_pr"])) + "}", 'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["knn"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["knn"])) + "}",'Non-linear': "NA",'Sparsity': "NA",'Dimensionality': "NA"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (IAMB)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_iamb_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_iamb_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_iamb_linear_dict_scores["knn_d"])) + "}", 'Non-linear': "NA",'Sparsity': "NA", 'Dimensionality': "NA"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["dt"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["dt"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["dt"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["dt"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["dt"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["dt"])) + "}",'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["dt"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["dt"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["dt_e"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["dt_e"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["dt_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["dt_e"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["rf"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["rf"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["rf"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["rf"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["rf"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["rf"])) + "}",'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["rf"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["rf"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["rf_e"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["rf_e"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["rf_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["rf_e"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["lr"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["lr"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["lr"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["lr"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["lr"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["lr"])) + "}",'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["lr"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["lr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["lr_l1"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["lr_l1"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["lr_l1"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["lr_l2"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["lr_l2"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["lr_l2"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["lr_e"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["lr_e"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["lr_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["lr_e"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["nb"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["nb"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["nb"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["nb"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["nb"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["nb"])) + "}",'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["nb"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["nb"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["nb_m"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["nb_m"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["nb_m"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["nb_m"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["nb_g"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["nb_g"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["nb_g"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["nb_g"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["nb_c"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["nb_c"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["nb_c"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["nb_c"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["svm"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["svm"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["svm"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["svm"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["svm"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["svm"])) + "}",'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["svm"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["svm"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["svm_l"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["svm_l"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["svm_l"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["svm_l"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["svm_po"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["svm_po"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["svm_po"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["svm_po"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["svm_r"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["svm_r"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["svm_r"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["svm_r"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["svm_pr"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["svm_pr"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["svm_pr"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["knn"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["knn"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["knn"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["knn"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["knn"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["knn"])) + "}",'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["knn"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["knn"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (MMHC)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_mmhc_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_mmhc_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_mmhc_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(bnlearn_mmhc_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_mmhc_nonlinear_dict_scores["knn_d"])) + "," + str(max(bnlearn_mmhc_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(bnlearn_mmhc_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_mmhc_sparse_dict_scores["knn_d"])) + "," + str(max(bnlearn_mmhc_sparse_dict_scores["knn_d"])) + "}", 'Dimensionality': str(round(mean(bnlearn_mmhc_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_mmhc_dimension_dict_scores["knn_d"])) + "," + str(max(bnlearn_mmhc_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["dt"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["dt"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["dt"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["dt"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["dt"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["dt"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["dt"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["dt"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["dt_e"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["dt_e"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["dt_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["dt_e"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["rf"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["rf"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["rf"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["rf"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["rf"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["rf"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["rf"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["rf"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["rf_e"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["rf_e"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["rf_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["rf_e"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["lr"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["lr"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["lr"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["lr"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["lr"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["lr"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["lr"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["lr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["lr_l1"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["lr_l1"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["lr_l1"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["lr_l2"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["lr_l2"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["lr_l2"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["lr_e"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["lr_e"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["lr_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["lr_e"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["nb"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["nb"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["nb"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["nb"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["nb"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["nb"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["nb"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["nb"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["nb_m"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["nb_m"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["nb_m"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["nb_m"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["nb_g"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["nb_g"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["nb_g"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["nb_g"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["nb_c"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["nb_c"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["nb_c"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["nb_c"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["svm"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["svm"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["svm"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["svm"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["svm"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["svm"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["svm"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["svm"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["svm_l"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["svm_l"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["svm_l"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["svm_l"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["svm_po"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["svm_po"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["svm_po"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["svm_po"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["svm_r"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["svm_r"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["svm_r"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["svm_r"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["svm_pr"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["svm_pr"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["svm_pr"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["knn"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["knn"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["knn"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["knn"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["knn"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["knn"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["knn"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["knn"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (RSMAX2)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_rsmax2_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_rsmax2_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_rsmax2_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(bnlearn_rsmax2_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_rsmax2_nonlinear_dict_scores["knn_d"])) + "," + str(max(bnlearn_rsmax2_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(bnlearn_rsmax2_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_rsmax2_sparse_dict_scores["knn_d"])) + "," + str(max(bnlearn_rsmax2_sparse_dict_scores["knn_d"])) + "}", 'Dimensionality': str(round(mean(bnlearn_rsmax2_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_rsmax2_dimension_dict_scores["knn_d"])) + "," + str(max(bnlearn_rsmax2_dimension_dict_scores["knn_d"])) + "}"})

        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Decision Tree (gini)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["dt"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["dt"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["dt"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["dt"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["dt"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["dt"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["dt"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["dt"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["dt"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Decision Tree (entropy)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["dt_e"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["dt_e"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["dt_e"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["dt_e"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["dt_e"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["dt_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["dt_e"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["dt_e"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["dt_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Random Forest (gini)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["rf"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["rf"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["rf"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["rf"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["rf"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["rf"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["rf"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["rf"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["rf"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Random Forest (entropy)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["rf_e"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["rf_e"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["rf_e"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["rf_e"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["rf_e"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["rf_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["rf_e"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["rf_e"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["rf_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Logistic Regression (penalty-none)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["lr"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["lr"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["lr"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["lr"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["lr"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["lr"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["lr"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["lr"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["lr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Logistic Regression (l1)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["lr_l1"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["lr_l1"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["lr_l1"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["lr_l1"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["lr_l1"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["lr_l1"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["lr_l1"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["lr_l1"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Logistic Regression (l2)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["lr_l2"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["lr_l2"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["lr_l2"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["lr_l2"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["lr_l2"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["lr_l2"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["lr_l2"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["lr_l2"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Logistic Regression (elasticnet)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["lr_e"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["lr_e"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["lr_e"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["lr_e"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["lr_e"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["lr_e"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["lr_e"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["lr_e"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["lr_e"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Naive Bayes (Bernoulli)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["nb"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["nb"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["nb"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["nb"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["nb"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["nb"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["nb"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["nb"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["nb"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Naive Bayes (Multinomial)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["nb_m"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["nb_m"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["nb_m"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["nb_m"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["nb_m"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["nb_m"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["nb_m"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["nb_m"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["nb_m"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Naive Bayes (Gaussian)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["nb_g"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["nb_g"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["nb_g"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["nb_g"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["nb_g"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["nb_g"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["nb_g"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["nb_g"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["nb_g"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Naive Bayes (Complement)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["nb_c"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["nb_c"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["nb_c"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["nb_c"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["nb_c"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["nb_c"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["nb_c"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["nb_c"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["nb_c"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Support Vector Machines (sigmoid)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["svm"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["svm"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["svm"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["svm"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["svm"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["svm"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["svm"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["svm"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["svm"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Support Vector Machines (linear)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["svm_l"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["svm_l"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["svm_l"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["svm_l"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["svm_l"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["svm_l"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["svm_l"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["svm_l"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["svm_l"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Support Vector Machines (poly)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["svm_po"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["svm_po"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["svm_po"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["svm_po"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["svm_po"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["svm_po"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["svm_po"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["svm_po"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["svm_po"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Support Vector Machines (rbf)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["svm_r"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["svm_r"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["svm_r"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["svm_r"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["svm_r"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["svm_r"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["svm_r"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["svm_r"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["svm_r"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'Support Vector Machines (precomputed)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["svm_pr"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["svm_pr"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["svm_pr"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["svm_pr"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["svm_pr"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["svm_pr"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["svm_pr"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["svm_pr"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'K Nearest Neighbor (uniform)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["knn"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["knn"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["knn"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["knn"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["knn"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["knn"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["knn"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["knn"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["knn"])) + "}"})
        #thewriter.writerow({'Algorithm': 'BN LEARN (H2PC)', 'Model': 'K Nearest Neighbor (distance)','Linear': str(round(mean(bnlearn_h2pc_linear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_h2pc_linear_dict_scores["knn_d"])) + "," + str(max(bnlearn_h2pc_linear_dict_scores["knn_d"])) + "}",'Non-linear': str(round(mean(bnlearn_h2pc_nonlinear_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_h2pc_nonlinear_dict_scores["knn_d"])) + "," + str(max(bnlearn_h2pc_nonlinear_dict_scores["knn_d"])) + "}",'Sparsity': str(round(mean(bnlearn_h2pc_sparse_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_h2pc_sparse_dict_scores["knn_d"])) + "," + str(max(bnlearn_h2pc_sparse_dict_scores["knn_d"])) + "}", 'Dimensionality': str(round(mean(bnlearn_h2pc_dimension_dict_scores["knn_d"]), 2)) + " {" + str(min(bnlearn_h2pc_dimension_dict_scores["knn_d"])) + "," + str(max(bnlearn_h2pc_dimension_dict_scores["knn_d"])) + "}"})


write_learned_to_csv()

def write_real_to_csv():
    experiments = ['Model', 'Linear', 'Non-linear', 'Sparsity', 'Dimensionality']
    with open('real_experiments_summary.csv', 'w', newline='') as csvfile:
        fieldnames = ['Model', 'Linear', 'Non-linear', 'Sparsity', 'Dimensionality']
        thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)
        thewriter.writeheader()
        #thewriter.writerow({'Model': 'Decision Tree (gini)','Linear': str(mean(real_linear_dt_scores))+" {"+str(min(real_linear_dt_scores))+","+str(max(real_linear_dt_scores))+"}", 'Non-linear': str(mean(real_nonlinear_dt_scores))+" {"+str(min(real_nonlinear_dt_scores))+","+str(max(real_nonlinear_dt_scores))+"}", 'Sparsity': str(mean(real_sparse_dt_scores))+" {"+str(min(real_sparse_dt_scores))+","+str(max(real_sparse_dt_scores))+"}", 'Dimensionality': str(mean(real_dimension_dt_scores))+" {"+str(min(real_dimension_dt_scores))+","+str(max(real_dimension_dt_scores))+"}"})
        #thewriter.writerow({'Model': 'Decision Tree (entropy)', 'Linear': str(mean(real_linear_dt_entropy_scores)) + " {" + str(min(real_linear_dt_entropy_scores)) + "," + str(max(real_linear_dt_entropy_scores)) + "}",'Non-linear': str(mean(real_nonlinear_dt_entropy_scores)) + " {" + str(min(real_nonlinear_dt_entropy_scores)) + "," + str(max(real_nonlinear_dt_entropy_scores)) + "}",'Sparsity': str(mean(real_sparse_dt_entropy_scores)) + " {" + str(min(real_sparse_dt_entropy_scores)) + "," + str(max(real_sparse_dt_entropy_scores)) + "}",'Dimensionality': str(mean(real_dimension_dt_entropy_scores)) + " {" + str(min(real_dimension_dt_entropy_scores)) + "," + str(max(real_dimension_dt_entropy_scores)) + "}"})
        #thewriter.writerow({'Model': 'Random Forest (gini)', 'Linear': str(mean(real_linear_rf_scores))+" {"+str(min(real_linear_rf_scores))+","+str(max(real_linear_rf_scores))+"}", 'Non-linear': str(mean(real_nonlinear_rf_scores))+" {"+str(min(real_nonlinear_rf_scores))+","+str(max(real_nonlinear_rf_scores))+"}", 'Sparsity': str(mean(real_sparse_rf_scores))+" {"+str(min(real_sparse_rf_scores))+","+str(max(real_sparse_rf_scores))+"}", 'Dimensionality': str(mean(real_dimension_rf_scores))+" {"+str(min(real_dimension_rf_scores))+","+str(max(real_dimension_rf_scores))+"}"})
        #thewriter.writerow({'Model': 'Random Forest (entropy)', 'Linear': str(mean(real_linear_rf_entropy_scores)) + " {" + str(min(real_linear_rf_entropy_scores)) + "," + str(max(real_linear_rf_entropy_scores)) + "}",'Non-linear': str(mean(real_nonlinear_rf_entropy_scores)) + " {" + str(min(real_nonlinear_rf_entropy_scores)) + "," + str(max(real_nonlinear_rf_entropy_scores)) + "}",'Sparsity': str(mean(real_sparse_rf_entropy_scores)) + " {" + str(min(real_sparse_rf_entropy_scores)) + "," + str(max(real_sparse_rf_entropy_scores)) + "}",'Dimensionality': str(mean(real_dimension_rf_entropy_scores)) + " {" + str(min(real_dimension_rf_entropy_scores)) + "," + str(max(real_dimension_rf_entropy_scores)) + "}"})
        #thewriter.writerow({'Model': 'Logistic Regression (penalty-none)', 'Linear': str(mean(real_linear_lr_scores))+" {"+str(min(real_linear_lr_scores))+","+str(max(real_linear_lr_scores))+"}", 'Non-linear': str(mean(real_nonlinear_lr_scores))+" {"+str(min(real_nonlinear_lr_scores))+","+str(max(real_nonlinear_lr_scores))+"}", 'Sparsity': str(mean(real_sparse_lr_scores))+" {"+str(min(real_sparse_lr_scores))+","+str(max(real_sparse_lr_scores))+"}", 'Dimensionality': str(mean(real_dimension_lr_scores))+" {"+str(min(real_dimension_lr_scores))+","+str(max(real_dimension_lr_scores))+"}"})
        #thewriter.writerow({'Model': 'Logistic Regression (l1)', 'Linear': str(mean(real_linear_lr_l1_scores)) + " {" + str(min(real_linear_lr_l1_scores)) + "," + str(max(real_linear_lr_l1_scores)) + "}",'Non-linear': str(mean(real_nonlinear_lr_l1_scores)) + " {" + str(min(real_nonlinear_lr_l1_scores)) + "," + str(max(real_nonlinear_lr_l1_scores)) + "}",'Sparsity': str(mean(real_sparse_lr_l1_scores)) + " {" + str(min(real_sparse_lr_l1_scores)) + "," + str(max(real_sparse_lr_l1_scores)) + "}",'Dimensionality': str(mean(real_dimension_lr_l1_scores)) + " {" + str(min(real_dimension_lr_l1_scores)) + "," + str(max(real_dimension_lr_l1_scores)) + "}"})
        #thewriter.writerow({'Model': 'Logistic Regression (l2)', 'Linear': str(mean(real_linear_lr_l2_scores)) + " {" + str(min(real_linear_lr_l2_scores)) + "," + str(max(real_linear_lr_l2_scores)) + "}",'Non-linear': str(mean(real_nonlinear_lr_l2_scores)) + " {" + str(min(real_nonlinear_lr_l2_scores)) + "," + str(max(real_nonlinear_lr_l2_scores)) + "}",'Sparsity': str(mean(real_sparse_lr_l2_scores)) + " {" + str(min(real_sparse_lr_l2_scores)) + "," + str(max(real_sparse_lr_l2_scores)) + "}",'Dimensionality': str(mean(real_dimension_lr_l2_scores)) + " {" + str(min(real_dimension_lr_l2_scores)) + "," + str(max(real_dimension_lr_l2_scores)) + "}"})
        #thewriter.writerow({'Model': 'Logistic Regression (elasticnet)', 'Linear': str(mean(real_linear_lr_elastic_scores)) + " {" + str(min(real_linear_lr_elastic_scores)) + "," + str(max(real_linear_lr_elastic_scores)) + "}",'Non-linear': str(mean(real_nonlinear_lr_elastic_scores)) + " {" + str(min(real_nonlinear_lr_elastic_scores)) + "," + str(max(real_nonlinear_lr_elastic_scores)) + "}",'Sparsity': str(mean(real_sparse_lr_elastic_scores)) + " {" + str(min(real_sparse_lr_elastic_scores)) + "," + str(max(real_sparse_lr_elastic_scores)) + "}",'Dimensionality': str(mean(real_dimension_lr_elastic_scores)) + " {" + str(min(real_dimension_lr_elastic_scores)) + "," + str(max(real_dimension_lr_elastic_scores)) + "}"})
        #thewriter.writerow({'Model': 'Naive Bayes (Bernoulli)', 'Linear': str(mean(real_linear_gb_scores))+" {"+str(min(real_linear_gb_scores))+","+str(max(real_linear_gb_scores))+"}",'Non-linear': str(mean(real_nonlinear_gb_scores))+" {"+str(min(real_nonlinear_gb_scores))+","+str(max(real_nonlinear_gb_scores))+"}", 'Sparsity': str(mean(real_sparse_gb_scores))+" {"+str(min(real_sparse_gb_scores))+","+str(max(real_sparse_gb_scores))+"}", 'Dimensionality': str(mean(real_dimension_gb_scores))+" {"+str(min(real_dimension_gb_scores))+","+str(max(real_dimension_gb_scores))+"}"})
        #thewriter.writerow({'Model': 'Naive Bayes (Multinomial)', 'Linear': str(mean(real_linear_gb_multi_scores)) + " {" + str(min(real_linear_gb_multi_scores)) + "," + str(max(real_linear_gb_multi_scores)) + "}",'Non-linear': str(mean(real_nonlinear_gb_multi_scores)) + " {" + str(min(real_nonlinear_gb_multi_scores)) + "," + str(max(real_nonlinear_gb_multi_scores)) + "}",'Sparsity': str(mean(real_sparse_gb_multi_scores)) + " {" + str(min(real_sparse_gb_multi_scores)) + "," + str(max(real_sparse_gb_multi_scores)) + "}",'Dimensionality': str(mean(real_dimension_gb_multi_scores)) + " {" + str(min(real_dimension_gb_multi_scores)) + "," + str(max(real_dimension_gb_multi_scores)) + "}"})
        #thewriter.writerow({'Model': 'Naive Bayes (Gaussian)','Linear': str(mean(real_linear_gb_gaussian_scores)) + " {" + str(min(real_linear_gb_gaussian_scores)) + "," + str(max(real_linear_gb_gaussian_scores)) + "}",'Non-linear': str(mean(real_nonlinear_gb_gaussian_scores)) + " {" + str(min(real_nonlinear_gb_gaussian_scores)) + "," + str(max(real_nonlinear_gb_gaussian_scores)) + "}",'Sparsity': str(mean(real_sparse_gb_gaussian_scores)) + " {" + str(min(real_sparse_gb_gaussian_scores)) + "," + str(max(real_sparse_gb_gaussian_scores)) + "}",'Dimensionality': str(mean(real_dimension_gb_gaussian_scores)) + " {" + str(min(real_dimension_gb_gaussian_scores)) + "," + str(max(real_dimension_gb_gaussian_scores)) + "}"})
        #thewriter.writerow({'Model': 'Naive Bayes (Complement)','Linear': str(mean(real_linear_gb_complement_scores)) + " {" + str(min(real_linear_gb_complement_scores)) + "," + str(max(real_linear_gb_complement_scores)) + "}",'Non-linear': str(mean(real_nonlinear_gb_complement_scores)) + " {" + str(min(real_nonlinear_gb_complement_scores)) + "," + str(max(real_nonlinear_gb_complement_scores)) + "}",'Sparsity': str(mean(real_sparse_gb_complement_scores)) + " {" + str(min(real_sparse_gb_complement_scores)) + "," + str(max(real_sparse_gb_complement_scores)) + "}",'Dimensionality': str(mean(real_dimension_gb_complement_scores)) + " {" + str(min(real_dimension_gb_complement_scores)) + "," + str(max(real_dimension_gb_complement_scores)) + "}"})
        #thewriter.writerow({'Model': 'Support Vector Machines (sigmoid)', 'Linear': str(mean(real_linear_svm_scores))+" {"+str(min(real_linear_svm_scores))+","+str(max(real_linear_svm_scores))+"}",'Non-linear': str(mean(real_nonlinear_svm_scores))+" {"+str(min(real_nonlinear_svm_scores))+","+str(max(real_nonlinear_svm_scores))+"}", 'Sparsity': str(mean(real_sparse_svm_scores))+" {"+str(min(real_sparse_svm_scores))+","+str(max(real_sparse_svm_scores))+"}", 'Dimensionality': str(mean(real_dimension_svm_scores))+" {"+str(min(real_dimension_svm_scores))+","+str(max(real_dimension_svm_scores))+"}"})
        #thewriter.writerow({'Model': 'Support Vector Machines (linear)','Linear': str(mean(real_linear_svm_linear_scores)) + " {" + str(min(real_linear_svm_linear_scores)) + "," + str(max(real_linear_svm_linear_scores)) + "}",'Non-linear': str(mean(real_nonlinear_svm_linear_scores)) + " {" + str(min(real_nonlinear_svm_linear_scores)) + "," + str(max(real_nonlinear_svm_linear_scores)) + "}",'Sparsity': str(mean(real_sparse_svm_linear_scores)) + " {" + str(min(real_sparse_svm_linear_scores)) + "," + str(max(real_sparse_svm_linear_scores)) + "}",'Dimensionality': str(mean(real_dimension_svm_linear_scores)) + " {" + str(min(real_dimension_svm_linear_scores)) + "," + str(max(real_dimension_svm_linear_scores)) + "}"})
        #thewriter.writerow({'Model': 'Support Vector Machines (poly)','Linear': str(mean(real_linear_svm_poly_scores)) + " {" + str(min(real_linear_svm_poly_scores)) + "," + str(max(real_linear_svm_poly_scores)) + "}",'Non-linear': str(mean(real_nonlinear_svm_poly_scores)) + " {" + str(min(real_nonlinear_svm_poly_scores)) + "," + str(max(real_nonlinear_svm_poly_scores)) + "}",'Sparsity': str(mean(real_sparse_svm_poly_scores)) + " {" + str(min(real_sparse_svm_poly_scores)) + "," + str(max(real_sparse_svm_poly_scores)) + "}",'Dimensionality': str(mean(real_dimension_svm_poly_scores)) + " {" + str(min(real_dimension_svm_poly_scores)) + "," + str(max(real_dimension_svm_poly_scores)) + "}"})
        #thewriter.writerow({'Model': 'Support Vector Machines (rbf)','Linear': str(mean(real_linear_svm_rbf_scores)) + " {" + str(min(real_linear_svm_rbf_scores)) + "," + str(max(real_linear_svm_rbf_scores)) + "}",'Non-linear': str(mean(real_nonlinear_svm_rbf_scores)) + " {" + str(min(real_nonlinear_svm_rbf_scores)) + "," + str(max(real_nonlinear_svm_rbf_scores)) + "}",'Sparsity': str(mean(real_sparse_svm_rbf_scores)) + " {" + str(min(real_sparse_svm_rbf_scores)) + "," + str(max(real_sparse_svm_rbf_scores)) + "}",'Dimensionality': str(mean(real_dimension_svm_rbf_scores)) + " {" + str(min(real_dimension_svm_rbf_scores)) + "," + str(max(real_dimension_svm_rbf_scores)) + "}"})
        #thewriter.writerow({'Model': 'Support Vector Machines (precomputed)','Linear': str(mean(real_linear_svm_precomputed_scores)) + " {" + str(min(real_linear_svm_precomputed_scores)) + "," + str(max(real_linear_svm_precomputed_scores)) + "}",'Non-linear': str(mean(real_nonlinear_svm_precomputed_scores)) + " {" + str(min(real_nonlinear_svm_precomputed_scores)) + "," + str(max(real_nonlinear_svm_precomputed_scores)) + "}",'Sparsity': str(mean(real_sparse_svm_precomputed_scores)) + " {" + str(min(real_sparse_svm_precomputed_scores)) + "," + str(max(real_sparse_svm_precomputed_scores)) + "}",'Dimensionality': str(mean(real_dimension_svm_precomputed_scores)) + " {" + str(min(real_dimension_svm_precomputed_scores)) + "," + str(max(real_dimension_svm_precomputed_scores)) + "}"})
        #thewriter.writerow({'Model': 'K Nearest Neighbor (uniform)', 'Linear': str(mean(real_linear_knn_scores))+" {"+str(min(real_linear_knn_scores))+","+str(max(real_linear_knn_scores))+"}",'Non-linear': str(mean(real_nonlinear_knn_scores))+" {"+str(min(real_nonlinear_knn_scores))+","+str(max(real_nonlinear_knn_scores))+"}", 'Sparsity': str(mean(real_sparse_knn_scores))+" {"+str(min(real_sparse_knn_scores))+","+str(max(real_sparse_knn_scores))+"}", 'Dimensionality': str(mean(real_dimension_knn_scores))+" {"+str(min(real_dimension_knn_scores))+","+str(max(real_dimension_knn_scores))+"}"})
        #thewriter.writerow({'Model': 'K Nearest Neighbor (distance)', 'Linear': str(mean(real_linear_knn_distance_scores))+" {"+str(min(real_linear_knn_distance_scores))+","+str(max(real_linear_knn_distance_scores))+"}",'Non-linear': str(mean(real_nonlinear_knn_distance_scores))+" {"+str(min(real_nonlinear_knn_distance_scores))+","+str(max(real_nonlinear_knn_distance_scores))+"}", 'Sparsity': str(mean(real_sparse_knn_distance_scores))+" {"+str(min(real_sparse_knn_distance_scores))+","+str(max(real_sparse_knn_distance_scores))+"}", 'Dimensionality': str(mean(real_dimension_knn_distance_scores))+" {"+str(min(real_dimension_knn_distance_scores))+","+str(max(real_dimension_knn_distance_scores))+"}"})

write_real_to_csv()

def write_real_to_figures():
    # Group by figure
    labels = ['DT_G', 'DT_E', 'RF_G', 'RF_E', 'LR', 'LR_L1', 'LR_L2', 'LR_E', 'NB_B', 'NB_G', 'NB_M', 'NB_C', 'SVM_S',
              'SVM_P', 'SVM_R', 'KNN_W', 'KNN_D']
    #bn_means = [round(mean(bnlearn_linear_dict_scores["dt"]),2), round(mean(bnlearn_linear_dict_scores["dt_e"]),2), round(mean(bnlearn_linear_dict_scores["rf"]),2), round(mean(bnlearn_linear_dict_scores["rf_e"]),2), round(mean(bnlearn_linear_dict_scores["lr"]),2), round(mean(bnlearn_linear_dict_scores["lr_l1"]),2), round(mean(bnlearn_linear_dict_scores["lr_l2"]),2), round(mean(bnlearn_linear_dict_scores["lr_e"]),2), round(mean(bnlearn_linear_dict_scores["nb"]),2), round(mean(bnlearn_linear_dict_scores["nb_g"]),2), round(mean(bnlearn_linear_dict_scores["nb_m"]),2), round(mean(bnlearn_linear_dict_scores["nb_c"]),2), round(mean(bnlearn_linear_dict_scores["svm"]),2), round(mean(bnlearn_linear_dict_scores["svm_po"]),2), round(mean(bnlearn_linear_dict_scores["svm_r"]),2), round(mean(bnlearn_linear_dict_scores["knn"]),2), round(mean(bnlearn_linear_dict_scores["knn_d"]),2)]
    #nt_means = [round(mean(notears_linear_dict_scores["dt"]),2), round(mean(notears_linear_dict_scores["dt_e"]),2), round(mean(notears_linear_dict_scores["rf"]),2), round(mean(notears_linear_dict_scores["rf_e"]),2), round(mean(notears_linear_dict_scores["lr"]),2), round(mean(notears_linear_dict_scores["lr_l1"]),2), round(mean(notears_linear_dict_scores["lr_l2"]),2), round(mean(notears_linear_dict_scores["lr_e"]),2), round(mean(notears_linear_dict_scores["nb"]),2), round(mean(notears_linear_dict_scores["nb_g"]),2), round(mean(notears_linear_dict_scores["nb_m"]),2), round(mean(notears_linear_dict_scores["nb_c"]),2), round(mean(notears_linear_dict_scores["svm"]),2), round(mean(notears_linear_dict_scores["svm_po"]),2), round(mean(notears_linear_dict_scores["svm_r"]),2), round(mean(notears_linear_dict_scores["knn"]),2), round(mean(notears_linear_dict_scores["knn_d"]),2)]

    x = np.arange(len(labels))  # the label locations
    width = 2  # the width of the bars

    fig, ax = plt.subplots()
    #rects1 = ax.bar(x - width / 2, bn_means, width, label='NO')
    #rects2 = ax.bar(x + width / 2, nt_means, width, label='NO_TEARS')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_ylabel('Accuracy')
    ax.set_title('Linear Problem - Performance by library on ML technique')
    ax.set_xticks(x, labels)
    ax.legend()

    #ax.bar_label(rects1, padding=3)
    #ax.bar_label(rects2, padding=3)

    fig.set_size_inches(15, 15)
    fig.tight_layout()
    plt.savefig('pipeline_summary_benchmark_by_library_bargraph.png')
    plt.show()

#write_real_to_figures()

def prediction_real_learned():
    #List with values and loop

    print("#### SimCal Real/Learned-world Predictions ####")

    print("-- Exact (1-1) max(rank) output")
    #real_linear_workflows = {'Decision Tree (gini)': max(real_linear_dt_scores), 'Decision Tree (entropy)': max(real_linear_dt_entropy_scores), 'Random Forest (gini)': max(real_linear_rf_scores), 'Random Forest (entropy)': max(real_linear_rf_entropy_scores),'Logistic Regression (none)': max(real_linear_lr_scores), 'Logistic Regression (l1)': max(real_linear_lr_l1_scores), 'Logistic Regression (l2)': max(real_linear_lr_l2_scores), 'Logistic Regression (elasticnet)': max(real_linear_lr_elastic_scores), 'Naive Bayes (bernoulli)': max(real_linear_gb_scores), 'Naive Bayes (multinomial)': max(real_linear_gb_multi_scores), 'Naive Bayes (gaussian)': max(real_linear_gb_gaussian_scores), 'Naive Bayes (complement)': max(real_linear_gb_complement_scores), 'Support Vector Machine (sigmoid)': max(real_linear_svm_scores), 'Support Vector Machine (polynomial)': max(real_linear_svm_poly_scores), 'Support Vector Machine (rbf)': max(real_linear_svm_rbf_scores), 'K Nearest Neighbor (uniform)': max(real_linear_knn_scores), 'K Nearest Neighbor (distance)': max(real_linear_knn_distance_scores)}
    #top_real_linear = max(real_linear_workflows, key=real_linear_workflows.get)
    #print("Real world - Linear problem, Prediction: ", top_real_linear)
    #sim_linear_workflows = {'BN Decision Tree (gini)': max(bnlearn_linear_dict_scores["dt"]), 'BN Decision Tree (entropy)': max(bnlearn_linear_dict_scores["dt_e"]),'NT Decision Tree (gini)': max(notears_linear_dict_scores["dt"]),'NT Decision Tree (entropy)': max(notears_linear_dict_scores["dt_e"]), 'BN Random Forest (gini)': max(bnlearn_linear_dict_scores["rf"]), 'BN Random Forest (entropy)': max(bnlearn_linear_dict_scores["rf_e"]),'NT Random Forest (gini)': max(notears_linear_dict_scores["rf"]),'NT Random Forest (entropy)': max(notears_linear_dict_scores["rf_e"]),'BN Logistic Regression (none)': max(bnlearn_linear_dict_scores["lr"]),'BN Logistic Regression (l1)': max(bnlearn_linear_dict_scores["lr_l1"]),'BN Logistic Regression (l2)': max(bnlearn_linear_dict_scores["lr_l2"]),'BN Logistic Regression (elastic)': max(bnlearn_linear_dict_scores["lr_e"]), 'NT Logistic Regression (none)': max(notears_linear_dict_scores["lr"]),  'NT Logistic Regression (l1)': max(notears_linear_dict_scores["lr_l1"]), 'NT Logistic Regression (l2)': max(notears_linear_dict_scores["lr_l2"]), 'NT Logistic Regression (elastic)': max(notears_linear_dict_scores["lr_e"]),'BN Naive Bayes (bernoulli)': max(bnlearn_linear_dict_scores["nb"]),'BN Naive Bayes (gaussian)': max(bnlearn_linear_dict_scores["nb_g"]),'BN Naive Bayes (multinomial)': max(bnlearn_linear_dict_scores["nb_m"]),'BN Naive Bayes (complement)': max(bnlearn_linear_dict_scores["nb_c"]), 'NT Naive Bayes (bernoulli)': max(notears_linear_dict_scores["nb"]),'NT Naive Bayes (gaussian)': max(notears_linear_dict_scores["nb_g"]),'NT Naive Bayes (multinomial)': max(notears_linear_dict_scores["nb_m"]),'NT Naive Bayes (complement)': max(notears_linear_dict_scores["nb_c"]), 'BN Support Vector Machine (sigmoid)': max(bnlearn_linear_dict_scores["svm"]), 'BN Support Vector Machine (polynomial)': max(bnlearn_linear_dict_scores["svm_po"]), 'BN Support Vector Machine (rbf)': max(bnlearn_linear_dict_scores["svm_r"]), 'NT Support Vector Machine (sigmoid)': max(notears_linear_dict_scores["svm"]),'NT Support Vector Machine (polynomial)': max(notears_linear_dict_scores["svm_po"]),'NT Support Vector Machine (rbf)': max(notears_linear_dict_scores["svm_r"]), 'BN K Nearest Neighbor (weight)': max(bnlearn_linear_dict_scores["knn"]),'BN K Nearest Neighbor (distance)': max(bnlearn_linear_dict_scores["knn_d"]),'NT K Nearest Neighbor (weight)': max(notears_linear_dict_scores["knn"]), 'NT K Nearest Neighbor (distance)': max(notears_linear_dict_scores["knn_d"])}
    #top_learned_linear = max(sim_linear_workflows, key=sim_linear_workflows.get)
    #print("Learned world - Linear problem, Prediction: ", top_learned_linear)

    #real_nonlinear_workflows = {'Decision Tree (gini)': max(real_nonlinear_dt_scores),
    #                         'Decision Tree (entropy)': max(real_nonlinear_dt_entropy_scores),
    #                         'Random Forest (gini)': max(real_nonlinear_rf_scores),
    #                         'Random Forest (entropy)': max(real_nonlinear_rf_entropy_scores),
    #                         'Logistic Regression (none)': max(real_nonlinear_lr_scores),
    #                         'Logistic Regression (l1)': max(real_nonlinear_lr_l1_scores),
    #                         'Logistic Regression (l2)': max(real_nonlinear_lr_l2_scores),
    #                         'Logistic Regression (elasticnet)': max(real_nonlinear_lr_elastic_scores),
    #                         'Naive Bayes (bernoulli)': max(real_nonlinear_gb_scores),
    #                         'Naive Bayes (multinomial)': max(real_nonlinear_gb_multi_scores),
    #                         'Naive Bayes (gaussian)': max(real_nonlinear_gb_gaussian_scores),
    #                         'Naive Bayes (complement)': max(real_nonlinear_gb_complement_scores),
    #                         'Support Vector Machine (sigmoid)': max(real_nonlinear_svm_scores),
    #                         'Support Vector Machine (polynomial)': max(real_nonlinear_svm_poly_scores),
    #                         'Support Vector Machine (rbf)': max(real_nonlinear_svm_rbf_scores),
    #                         'K Nearest Neighbor (uniform)': max(real_nonlinear_knn_scores),
    #                         'K Nearest Neighbor (distance)': max(real_nonlinear_knn_distance_scores)}
    #top_real_nonlinear = max(real_nonlinear_workflows, key=real_nonlinear_workflows.get)
    #print("Real world - Nonlinear problem, Prediction: ", top_real_nonlinear)
    #sim_nonlinear_workflows = {'BN Decision Tree (gini)': max(bnlearn_nonlinear_dict_scores["dt"]),
    #                        'BN Decision Tree (entropy)': max(bnlearn_nonlinear_dict_scores["dt_e"]),
    #                        'NT Decision Tree (gini)': max(notears_nonlinear_dict_scores["dt"]),
    #                        'NT Decision Tree (entropy)': max(notears_nonlinear_dict_scores["dt_e"]),
    #                        'BN Random Forest (gini)': max(bnlearn_nonlinear_dict_scores["rf"]),
    #                        'BN Random Forest (entropy)': max(bnlearn_nonlinear_dict_scores["rf_e"]),
    #                        'NT Random Forest (gini)': max(notears_nonlinear_dict_scores["rf"]),
    #                        'NT Random Forest (entropy)': max(notears_nonlinear_dict_scores["rf_e"]),
    #                        'BN Logistic Regression (none)': max(bnlearn_nonlinear_dict_scores["lr"]),
    #                        'BN Logistic Regression (l1)': max(bnlearn_nonlinear_dict_scores["lr_l1"]),
    #                        'BN Logistic Regression (l2)': max(bnlearn_nonlinear_dict_scores["lr_l2"]),
    #                        'BN Logistic Regression (elastic)': max(bnlearn_nonlinear_dict_scores["lr_e"]),
    #                        'NT Logistic Regression (none)': max(notears_nonlinear_dict_scores["lr"]),
    #                        'NT Logistic Regression (l1)': max(notears_nonlinear_dict_scores["lr_l1"]),
    #                        'NT Logistic Regression (l2)': max(notears_nonlinear_dict_scores["lr_l2"]),
    #                        'NT Logistic Regression (elastic)': max(notears_nonlinear_dict_scores["lr_e"]),
    #                        'BN Naive Bayes (bernoulli)': max(bnlearn_nonlinear_dict_scores["nb"]),
    #                        'BN Naive Bayes (gaussian)': max(bnlearn_nonlinear_dict_scores["nb_g"]),
    #                        'BN Naive Bayes (multinomial)': max(bnlearn_nonlinear_dict_scores["nb_m"]),
    #                        'BN Naive Bayes (complement)': max(bnlearn_nonlinear_dict_scores["nb_c"]),
    #                        'NT Naive Bayes (bernoulli)': max(notears_nonlinear_dict_scores["nb"]),
    #                        'NT Naive Bayes (gaussian)': max(notears_nonlinear_dict_scores["nb_g"]),
    #                        'NT Naive Bayes (multinomial)': max(notears_nonlinear_dict_scores["nb_m"]),
    #                        'NT Naive Bayes (complement)': max(notears_nonlinear_dict_scores["nb_c"]),
    #                        'BN Support Vector Machine (sigmoid)': max(bnlearn_nonlinear_dict_scores["svm"]),
    #                        'BN Support Vector Machine (polynomial)': max(bnlearn_nonlinear_dict_scores["svm_po"]),
    #                        'BN Support Vector Machine (rbf)': max(bnlearn_nonlinear_dict_scores["svm_r"]),
    #                        'NT Support Vector Machine (sigmoid)': max(notears_nonlinear_dict_scores["svm"]),
    #                        'NT Support Vector Machine (polynomial)': max(notears_nonlinear_dict_scores["svm_po"]),
    #                        'NT Support Vector Machine (rbf)': max(notears_nonlinear_dict_scores["svm_r"]),
    #                        'BN K Nearest Neighbor (weight)': max(bnlearn_nonlinear_dict_scores["knn"]),
    #                        'BN K Nearest Neighbor (distance)': max(bnlearn_nonlinear_dict_scores["knn_d"]),
    #                        'NT K Nearest Neighbor (weight)': max(notears_nonlinear_dict_scores["knn"]),
    #                        'NT K Nearest Neighbor (distance)': max(notears_nonlinear_dict_scores["knn_d"])}
    #top_learned_nonlinear = max(sim_nonlinear_workflows, key=sim_nonlinear_workflows.get)
    #print("Learned world - Nonlinear problem, Prediction: ", top_learned_nonlinear)

    #real_sparse_workflows = {'Decision Tree (gini)': max(real_sparse_dt_scores),
    #                         'Decision Tree (entropy)': max(real_sparse_dt_entropy_scores),
    #                         'Random Forest (gini)': max(real_sparse_rf_scores),
    #                         'Random Forest (entropy)': max(real_sparse_rf_entropy_scores),
    #                         'Logistic Regression (none)': max(real_sparse_lr_scores),
    #                         'Logistic Regression (l1)': max(real_sparse_lr_l1_scores),
    #                         'Logistic Regression (l2)': max(real_sparse_lr_l2_scores),
    #                         'Logistic Regression (elasticnet)': max(real_sparse_lr_elastic_scores),
    #                         'Naive Bayes (bernoulli)': max(real_sparse_gb_scores),
    #                         'Naive Bayes (multinomial)': max(real_sparse_gb_multi_scores),
    #                         'Naive Bayes (gaussian)': max(real_sparse_gb_gaussian_scores),
    #                         'Naive Bayes (complement)': max(real_sparse_gb_complement_scores),
    #                         'Support Vector Machine (sigmoid)': max(real_sparse_svm_scores),
    #                         'Support Vector Machine (polynomial)': max(real_sparse_svm_poly_scores),
    #                         'Support Vector Machine (rbf)': max(real_sparse_svm_rbf_scores),
    #                         'K Nearest Neighbor (uniform)': max(real_sparse_knn_scores),
    #                         'K Nearest Neighbor (distance)': max(real_sparse_knn_distance_scores)}
    #top_real_sparse = max(real_sparse_workflows, key=real_sparse_workflows.get)
    #print("Real world - Sparse problem, Prediction: ", top_real_sparse)
    #sim_sparse_workflows = {'BN Decision Tree (gini)': max(bnlearn_sparse_dict_scores["dt"]),
    #                        'BN Decision Tree (entropy)': max(bnlearn_sparse_dict_scores["dt_e"]),
    #                        'NT Decision Tree (gini)': max(notears_sparse_dict_scores["dt"]),
    #                        'NT Decision Tree (entropy)': max(notears_sparse_dict_scores["dt_e"]),
    #                        'BN Random Forest (gini)': max(bnlearn_sparse_dict_scores["rf"]),
    #                        'BN Random Forest (entropy)': max(bnlearn_sparse_dict_scores["rf_e"]),
    #                        'NT Random Forest (gini)': max(notears_sparse_dict_scores["rf"]),
    #                        'NT Random Forest (entropy)': max(notears_sparse_dict_scores["rf_e"]),
    #                        'BN Logistic Regression (none)': max(bnlearn_sparse_dict_scores["lr"]),
    #                        'BN Logistic Regression (l1)': max(bnlearn_sparse_dict_scores["lr_l1"]),
    #                        'BN Logistic Regression (l2)': max(bnlearn_sparse_dict_scores["lr_l2"]),
    #                        'BN Logistic Regression (elastic)': max(bnlearn_sparse_dict_scores["lr_e"]),
    #                        'NT Logistic Regression (none)': max(notears_sparse_dict_scores["lr"]),
    #                        'NT Logistic Regression (l1)': max(notears_sparse_dict_scores["lr_l1"]),
    #                        'NT Logistic Regression (l2)': max(notears_sparse_dict_scores["lr_l2"]),
    #                        'NT Logistic Regression (elastic)': max(notears_sparse_dict_scores["lr_e"]),
    #                        'BN Naive Bayes (bernoulli)': max(bnlearn_sparse_dict_scores["nb"]),
    #                        'BN Naive Bayes (gaussian)': max(bnlearn_sparse_dict_scores["nb_g"]),
    #                        'BN Naive Bayes (multinomial)': max(bnlearn_sparse_dict_scores["nb_m"]),
    #                        'BN Naive Bayes (complement)': max(bnlearn_sparse_dict_scores["nb_c"]),
    #                        'NT Naive Bayes (bernoulli)': max(notears_sparse_dict_scores["nb"]),
    #                        'NT Naive Bayes (gaussian)': max(notears_sparse_dict_scores["nb_g"]),
    #                        'NT Naive Bayes (multinomial)': max(notears_sparse_dict_scores["nb_m"]),
    #                        'NT Naive Bayes (complement)': max(notears_sparse_dict_scores["nb_c"]),
    #                        'BN Support Vector Machine (sigmoid)': max(bnlearn_sparse_dict_scores["svm"]),
    #                        'BN Support Vector Machine (polynomial)': max(bnlearn_sparse_dict_scores["svm_po"]),
    #                        'BN Support Vector Machine (rbf)': max(bnlearn_sparse_dict_scores["svm_r"]),
    #                        'NT Support Vector Machine (sigmoid)': max(notears_sparse_dict_scores["svm"]),
    #                        'NT Support Vector Machine (polynomial)': max(notears_sparse_dict_scores["svm_po"]),
    #                        'NT Support Vector Machine (rbf)': max(notears_sparse_dict_scores["svm_r"]),
    #                        'BN K Nearest Neighbor (weight)': max(bnlearn_sparse_dict_scores["knn"]),
    #                        'BN K Nearest Neighbor (distance)': max(bnlearn_sparse_dict_scores["knn_d"]),
    #                        'NT K Nearest Neighbor (weight)': max(notears_sparse_dict_scores["knn"]),
    #                        'NT K Nearest Neighbor (distance)': max(notears_sparse_dict_scores["knn_d"])}
    #top_learned_sparse = max(sim_sparse_workflows, key=sim_sparse_workflows.get)
    #print("Learned world - Sparse problem, Prediction: ", top_learned_sparse)

    #real_dimension_workflows = {'Decision Tree (gini)': max(real_dimension_dt_scores),
    #                         'Decision Tree (entropy)': max(real_dimension_dt_entropy_scores),
    #                         'Random Forest (gini)': max(real_dimension_rf_scores),
    #                         'Random Forest (entropy)': max(real_dimension_rf_entropy_scores),
    #                         'Logistic Regression (none)': max(real_dimension_lr_scores),
    #                         'Logistic Regression (l1)': max(real_dimension_lr_l1_scores),
    #                         'Logistic Regression (l2)': max(real_dimension_lr_l2_scores),
    #                         'Logistic Regression (elasticnet)': max(real_dimension_lr_elastic_scores),
    #                         'Naive Bayes (bernoulli)': max(real_dimension_gb_scores),
    #                         'Naive Bayes (multinomial)': max(real_dimension_gb_multi_scores),
    #                         'Naive Bayes (gaussian)': max(real_dimension_gb_gaussian_scores),
    #                         'Naive Bayes (complement)': max(real_dimension_gb_complement_scores),
    #                         'Support Vector Machine (sigmoid)': max(real_dimension_svm_scores),
    #                         'Support Vector Machine (polynomial)': max(real_dimension_svm_poly_scores),
    #                         'Support Vector Machine (rbf)': max(real_dimension_svm_rbf_scores),
    #                         'K Nearest Neighbor (uniform)': max(real_dimension_knn_scores),
    #                         'K Nearest Neighbor (distance)': max(real_dimension_knn_distance_scores)}
    #top_real_dimension = max(real_dimension_workflows, key=real_dimension_workflows.get)
    #print("Real world - Dimensional problem, Prediction: ", top_real_dimension)
    #sim_dimension_workflows = {'BN Decision Tree (gini)': max(bnlearn_dimension_dict_scores["dt"]),
    #                        'BN Decision Tree (entropy)': max(bnlearn_dimension_dict_scores["dt_e"]),
    #                        'NT Decision Tree (gini)': max(notears_dimension_dict_scores["dt"]),
    #                        'NT Decision Tree (entropy)': max(notears_dimension_dict_scores["dt_e"]),
    #                        'BN Random Forest (gini)': max(bnlearn_dimension_dict_scores["rf"]),
    #                        'BN Random Forest (entropy)': max(bnlearn_dimension_dict_scores["rf_e"]),
    #                        'NT Random Forest (gini)': max(notears_dimension_dict_scores["rf"]),
    #                        'NT Random Forest (entropy)': max(notears_dimension_dict_scores["rf_e"]),
    #                        'BN Logistic Regression (none)': max(bnlearn_dimension_dict_scores["lr"]),
    #                        'BN Logistic Regression (l1)': max(bnlearn_dimension_dict_scores["lr_l1"]),
    #                        'BN Logistic Regression (l2)': max(bnlearn_dimension_dict_scores["lr_l2"]),
    #                        'BN Logistic Regression (elastic)': max(bnlearn_dimension_dict_scores["lr_e"]),
    #                        'NT Logistic Regression (none)': max(notears_dimension_dict_scores["lr"]),
    #                        'NT Logistic Regression (l1)': max(notears_dimension_dict_scores["lr_l1"]),
    #                        'NT Logistic Regression (l2)': max(notears_dimension_dict_scores["lr_l2"]),
    #                        'NT Logistic Regression (elastic)': max(notears_dimension_dict_scores["lr_e"]),
    #                        'BN Naive Bayes (bernoulli)': max(bnlearn_dimension_dict_scores["nb"]),
    #                        'BN Naive Bayes (gaussian)': max(bnlearn_dimension_dict_scores["nb_g"]),
    #                        'BN Naive Bayes (multinomial)': max(bnlearn_dimension_dict_scores["nb_m"]),
    #                        'BN Naive Bayes (complement)': max(bnlearn_dimension_dict_scores["nb_c"]),
    #                        'NT Naive Bayes (bernoulli)': max(notears_dimension_dict_scores["nb"]),
    #                        'NT Naive Bayes (gaussian)': max(notears_dimension_dict_scores["nb_g"]),
    #                        'NT Naive Bayes (multinomial)': max(notears_dimension_dict_scores["nb_m"]),
    #                        'NT Naive Bayes (complement)': max(notears_dimension_dict_scores["nb_c"]),
    #                        'BN Support Vector Machine (sigmoid)': max(bnlearn_dimension_dict_scores["svm"]),
    #                        'BN Support Vector Machine (polynomial)': max(bnlearn_dimension_dict_scores["svm_po"]),
    #                        'BN Support Vector Machine (rbf)': max(bnlearn_dimension_dict_scores["svm_r"]),
    #                        'NT Support Vector Machine (sigmoid)': max(notears_dimension_dict_scores["svm"]),
    #                        'NT Support Vector Machine (polynomial)': max(notears_dimension_dict_scores["svm_po"]),
    #                        'NT Support Vector Machine (rbf)': max(notears_dimension_dict_scores["svm_r"]),
    #                        'BN K Nearest Neighbor (weight)': max(bnlearn_dimension_dict_scores["knn"]),
    #                        'BN K Nearest Neighbor (distance)': max(bnlearn_dimension_dict_scores["knn_d"]),
    #                        'NT K Nearest Neighbor (weight)': max(notears_dimension_dict_scores["knn"]),
    #                        'NT K Nearest Neighbor (distance)': max(notears_dimension_dict_scores["knn_d"])}
    #top_learned_dimension = max(sim_dimension_workflows, key=sim_dimension_workflows.get)
    #print("Learned world - Dimensional problem, Prediction: ", top_learned_dimension)

    #print("Relative (point-based) rank output")

    #workflows = {'Decision Tree': real_nonlinear_dt, 'Random Forest': real_nonlinear_rf,
    #             'Logistic Regression': real_nonlinear_lr, 'Naive Bayes': real_nonlinear_gb,
    #             'Support Vector Machine': real_nonlinear_svm, 'K Nearest Neighbor': real_nonlinear_knn}
    #top_real = max(workflows, key=workflows.get)
    #print("Real world - Non-Linear ground truth, Prediction ", top_real)
    #workflows = {'BN Decision Tree': bnlearn_nonlinear_dt, 'NT Decision Tree': notears_nonlinear_dt,
    #             'BN Random Forest': bnlearn_nonlinear_rf, 'NT Random Forest': notears_nonlinear_rf,
    #             'BN Logistic Regression': bnlearn_nonlinear_lr, 'NT Logistic Regression': notears_nonlinear_lr,
    #             'BN Naive Bayes': bnlearn_nonlinear_nb, 'NT Naive Bayes': notears_nonlinear_nb,
    #             'BN Support Vector Machine': bnlearn_nonlinear_svm, 'NT Support Vector Machine': notears_nonlinear_svm,
    #             'BN K Nearest Neighbor': bnlearn_nonlinear_knn, 'NT K Nearest Neighbor': notears_nonlinear_knn}
    #top_learned = max(workflows, key=workflows.get)
    #print("Learned world - Non-Linear, Prediction ", top_learned)
    #workflows = {'Decision Tree': real_sparse_dt, 'Random Forest': real_sparse_rf,
    #             'Logistic Regression': real_sparse_lr, 'Naive Bayes': real_sparse_gb,
    #             'Support Vector Machine': real_sparse_svm, 'K Nearest Neighbor': real_sparse_knn}
    #top_real = max(workflows, key=workflows.get)
    #print("Real world - Sparse ground truth, Prediction ", top_real)
    #workflows = {'BN Decision Tree': bnlearn_sparse_dt, 'NT Decision Tree': notears_sparse_dt,
    #             'BN Random Forest': bnlearn_sparse_rf, 'NT Random Forest': notears_sparse_rf,
    #             'BN Logistic Regression': bnlearn_sparse_lr, 'NT Logistic Regression': notears_sparse_lr,
    #             'BN Naive Bayes': bnlearn_sparse_nb, 'NT Naive Bayes': notears_sparse_nb,
    #             'BN Support Vector Machine': bnlearn_sparse_svm, 'NT Support Vector Machine': notears_sparse_svm,
    #             'BN K Nearest Neighbor': bnlearn_sparse_knn, 'NT K Nearest Neighbor': notears_sparse_knn}
    #top_learned = max(workflows, key=workflows.get)
    #print("Learned world - Sparse, Prediction ", top_learned)

real_experiment_summary = pd.read_csv("real_experiments_summary.csv")
real_experiment_summary

learned_experiment_summary = pd.read_csv("simulation_experiments_summary.csv")
learned_experiment_summary

prediction_real_learned()
